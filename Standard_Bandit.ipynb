{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = reward_means\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.reward_means[arm])\n",
    "\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        for arm in range(self.num_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        ucb_values = self.values + self.c * np.sqrt(np.log(self.t) / self.counts)\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "class EXP3:\n",
    "    def __init__(self, num_arms, gamma):\n",
    "        self.num_arms = num_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + \\\n",
    "                       self.gamma / self.num_arms\n",
    "        return np.random.choice(self.num_arms, p=probabilities)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + \\\n",
    "                       self.gamma / self.num_arms\n",
    "        estimated_reward = reward / probabilities[arm]\n",
    "        self.weights[arm] *= np.exp(self.gamma * estimated_reward / self.num_arms)\n",
    "        self.t += 1\n",
    "\n",
    "def run_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    regrets = []\n",
    "    optimal_reward = max(env.reward_means)\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        bandit.update(arm, reward)\n",
    "        regret = optimal_reward - reward\n",
    "        regrets.append(regret)\n",
    "\n",
    "    return np.cumsum(regrets)\n",
    "\n",
    "# Experimental setup\n",
    "num_arms = 10\n",
    "reward_means = np.linspace(0, 1, num_arms)\n",
    "env = BanditEnvironment(num_arms, reward_means)\n",
    "num_steps = 10000\n",
    "\n",
    "# Run experiments\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# UCB experiment\n",
    "ucb_regret = run_experiment(UCB, {'num_arms': num_arms, 'c': 2}, env, num_steps)\n",
    "plt.plot(ucb_regret, label='UCB (c=2)')\n",
    "\n",
    "# EXP3 experiment\n",
    "exp3_regret = run_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, env, num_steps)\n",
    "plt.plot(exp3_regret, label='EXP3 (Î³=0.1)')\n",
    "\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "plt.title(\"UCB vs EXP3 Performance Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = reward_means\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.reward_means[arm])\n",
    "\n",
    "class DynamicBanditEnvironment(BanditEnvironment):\n",
    "    def __init__(self, num_arms, reward_means, volatility=0.01):\n",
    "        super().__init__(num_arms, reward_means)\n",
    "        self.volatility = volatility\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        reward = super().pull_arm(arm)  # Get reward first\n",
    "        # Update means with random walk\n",
    "        self.reward_means += np.random.normal(0, self.volatility, self.num_arms)\n",
    "        self.reward_means = np.clip(self.reward_means, 0, 1)\n",
    "        return reward\n",
    "\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        for arm in range(self.num_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        ucb_values = self.values + self.c * np.sqrt(np.log(self.t) / self.counts)\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "class EXP3:\n",
    "    def __init__(self, num_arms, gamma):\n",
    "        self.num_arms = num_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + \\\n",
    "                       self.gamma / self.num_arms\n",
    "        return np.random.choice(self.num_arms, p=probabilities)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + \\\n",
    "                       self.gamma / self.num_arms\n",
    "        estimated_reward = reward / probabilities[arm]\n",
    "        self.weights[arm] *= np.exp(self.gamma * estimated_reward / self.num_arms)\n",
    "        self.t += 1\n",
    "\n",
    "def run_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    regrets = []\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        current_optimal = max(env.reward_means)  # Get current optimal before pull\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        regret = current_optimal - reward\n",
    "        regrets.append(regret)\n",
    "        bandit.update(arm, reward)\n",
    "\n",
    "    return np.cumsum(regrets)\n",
    "\n",
    "# Experimental setup\n",
    "num_arms = 10\n",
    "base_reward_means = np.linspace(0, 1, num_arms)\n",
    "num_steps = 20000\n",
    "\n",
    "# Create environments\n",
    "static_env = BanditEnvironment(num_arms, base_reward_means.copy())\n",
    "dynamic_env = DynamicBanditEnvironment(num_arms, base_reward_means.copy(), volatility=0.01)\n",
    "\n",
    "# Run experiments\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Static Environment\n",
    "plt.subplot(1, 2, 1)\n",
    "ucb_static = run_experiment(UCB, {'num_arms': num_arms, 'c': 2}, static_env, num_steps)\n",
    "exp3_static = run_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, static_env, num_steps)\n",
    "plt.plot(ucb_static, label='UCB (Static)')\n",
    "plt.plot(exp3_static, label='EXP3 (Static)')\n",
    "plt.title(\"Static Environment\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "\n",
    "# Dynamic Environment\n",
    "plt.subplot(1, 2, 2)\n",
    "ucb_dynamic = run_experiment(UCB, {'num_arms': num_arms, 'c': 2}, dynamic_env, num_steps)\n",
    "exp3_dynamic = run_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, dynamic_env, num_steps)\n",
    "plt.plot(ucb_dynamic, label='UCB (Dynamic)')\n",
    "plt.plot(exp3_dynamic, label='EXP3 (Dynamic)')\n",
    "plt.title(\"Dynamic Environment\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic(shuffle after fixed steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = reward_means\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.reward_means[arm])\n",
    "\n",
    "class DynamicBanditEnvironment(BanditEnvironment):\n",
    "    def __init__(self, num_arms, reward_means, change_interval=2000):\n",
    "        super().__init__(num_arms, reward_means.copy())\n",
    "        self.change_interval = change_interval\n",
    "        self.time_step = 0\n",
    "        self.original_means = reward_means.copy()\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        self.time_step += 1\n",
    "        if self.time_step % self.change_interval == 0:\n",
    "            # Shuffle the reward means to abruptly change optimal arm\n",
    "            np.random.shuffle(self.reward_means)\n",
    "        return super().pull_arm(arm)\n",
    "\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        for arm in range(self.num_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        ucb_values = self.values + self.c * np.sqrt(np.log(self.t) / self.counts)\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "class EXP3:\n",
    "    def __init__(self, num_arms, gamma):\n",
    "        self.num_arms = num_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + \\\n",
    "                       self.gamma / self.num_arms\n",
    "        return np.random.choice(self.num_arms, p=probabilities)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + \\\n",
    "                       self.gamma / self.num_arms\n",
    "        estimated_reward = reward / probabilities[arm]\n",
    "        self.weights[arm] *= np.exp(self.gamma * estimated_reward / self.num_arms)\n",
    "        self.t += 1\n",
    "\n",
    "def run_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    regrets = []\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        current_optimal = max(env.reward_means)\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        regret = current_optimal - reward\n",
    "        regrets.append(regret)\n",
    "        bandit.update(arm, reward)\n",
    "\n",
    "    return np.cumsum(regrets)\n",
    "\n",
    "# Experimental setup\n",
    "num_arms = 10\n",
    "base_reward_means = np.linspace(0, 1, num_arms)\n",
    "num_steps = 10000\n",
    "\n",
    "# Create environments\n",
    "static_env = BanditEnvironment(num_arms, base_reward_means.copy())\n",
    "dynamic_env = DynamicBanditEnvironment(num_arms, base_reward_means.copy(), change_interval=2000)\n",
    "\n",
    "# Run experiments\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Static Environment\n",
    "plt.subplot(1, 2, 1)\n",
    "ucb_static = run_experiment(UCB, {'num_arms': num_arms, 'c': 2}, static_env, num_steps)\n",
    "exp3_static = run_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, static_env, num_steps)\n",
    "plt.plot(ucb_static, label='UCB (Static)')\n",
    "plt.plot(exp3_static, label='EXP3 (Static)')\n",
    "plt.title(\"Static Environment\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "\n",
    "# Dynamic Environment\n",
    "plt.subplot(1, 2, 2)\n",
    "ucb_dynamic = run_experiment(UCB, {'num_arms': num_arms, 'c': 2}, dynamic_env, num_steps)\n",
    "exp3_dynamic = run_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, dynamic_env, num_steps)\n",
    "plt.plot(ucb_dynamic, label='UCB (Dynamic)')\n",
    "plt.plot(exp3_dynamic, label='EXP3 (Dynamic)')\n",
    "plt.title(\"Dynamic Environment (Changes every 2000 steps)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = reward_means\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.reward_means[arm])\n",
    "\n",
    "class AdversarialBanditEnvironment(BanditEnvironment):\n",
    "    def __init__(self, num_arms, base_reward=0.7, attack_strength=0.02):\n",
    "        \"\"\"\n",
    "        Adversarial environment that actively penalizes successful arms\n",
    "        Args:\n",
    "            base_reward: Initial reward probability for all arms\n",
    "            attack_strength: How much to reduce reward when an arm is pulled\n",
    "        \"\"\"\n",
    "        super().__init__(num_arms, np.full(num_arms, base_reward))\n",
    "        self.base_reward = base_reward\n",
    "        self.attack_strength = attack_strength\n",
    "        self.pull_counts = np.zeros(num_arms)\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        # Calculate reward before updating probabilities\n",
    "        reward = super().pull_arm(arm)\n",
    "        \n",
    "        # Adversarial update rule:\n",
    "        self.pull_counts[arm] += 1\n",
    "        # Reduce reward probability for pulled arm\n",
    "        self.reward_means[arm] = max(0, self.reward_means[arm] - self.attack_strength)\n",
    "        # Gradually restore other arms' probabilities toward base reward\n",
    "        for a in range(self.num_arms):\n",
    "            if a != arm:\n",
    "                self.reward_means[a] = min(\n",
    "                    self.base_reward,\n",
    "                    self.reward_means[a] + self.attack_strength/(self.num_arms-1)\n",
    "                )\n",
    "        return reward\n",
    "\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        for arm in range(self.num_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        ucb_values = self.values + self.c * np.sqrt(np.log(self.t) / self.counts)\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "class EXP3:\n",
    "    def __init__(self, num_arms, gamma):\n",
    "        self.num_arms = num_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + \\\n",
    "                       self.gamma / self.num_arms\n",
    "        return np.random.choice(self.num_arms, p=probabilities)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + \\\n",
    "                       self.gamma / self.num_arms\n",
    "        estimated_reward = reward / probabilities[arm]\n",
    "        self.weights[arm] *= np.exp(self.gamma * estimated_reward / self.num_arms)\n",
    "        self.t += 1\n",
    "\n",
    "def run_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    regrets = []\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        current_optimal = max(env.reward_means)\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        regret = current_optimal - reward\n",
    "        regrets.append(regret)\n",
    "        bandit.update(arm, reward)\n",
    "\n",
    "    return np.cumsum(regrets)\n",
    "\n",
    "# Experimental setup\n",
    "num_arms = 10\n",
    "num_steps = 10000\n",
    "\n",
    "# Create environments\n",
    "static_env = BanditEnvironment(num_arms, np.full(num_arms, 0.7))\n",
    "adv_env = AdversarialBanditEnvironment(num_arms, base_reward=0.7, attack_strength=0.02)\n",
    "\n",
    "# Run experiments\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Static Environment\n",
    "plt.subplot(1, 2, 1)\n",
    "ucb_static = run_experiment(UCB, {'num_arms': num_arms, 'c': 2}, static_env, num_steps)\n",
    "exp3_static = run_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, static_env, num_steps)\n",
    "plt.plot(ucb_static, label='UCB (Static)')\n",
    "plt.plot(exp3_static, label='EXP3 (Static)')\n",
    "plt.title(\"Static Environment (All arms p=0.7)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "\n",
    "# Adversarial Environment\n",
    "plt.subplot(1, 2, 2)\n",
    "ucb_adv = run_experiment(UCB, {'num_arms': num_arms, 'c': 2}, adv_env, num_steps)\n",
    "exp3_adv = run_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, adv_env, num_steps)\n",
    "plt.plot(ucb_adv, label='UCB (Adversarial)')\n",
    "plt.plot(exp3_adv, label='EXP3 (Adversarial)')\n",
    "plt.title(\"Adversarial Environment\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        \"\"\"\n",
    "        Standard multi-armed bandit environment where each arm has a fixed probability of providing a reward.\n",
    "        :param num_arms: Number of arms in the bandit environment\n",
    "        :param reward_means: Array of reward probabilities for each arm\n",
    "        \"\"\"\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = np.array(reward_means)\n",
    "    \n",
    "    def pull_arm(self, arm):\n",
    "        \"\"\"\n",
    "        Pulls the specified arm and returns a reward (1 or 0) based on its probability.\n",
    "        :param arm: The selected arm\n",
    "        :return: Reward (1 if successful, 0 otherwise)\n",
    "        \"\"\"\n",
    "        return np.random.binomial(1, self.reward_means[arm])\n",
    "\n",
    "class AdversarialBanditEnvironment(BanditEnvironment):\n",
    "    def __init__(self, num_arms, base_reward=0.7, attack_strength=0.02):\n",
    "        \"\"\"\n",
    "        Adversarial environment that reduces the reward probability of frequently chosen arms.\n",
    "        :param num_arms: Number of arms\n",
    "        :param base_reward: Initial reward probability for all arms\n",
    "        :param attack_strength: Penalty applied to pulled arms\n",
    "        \"\"\"\n",
    "        super().__init__(num_arms, np.full(num_arms, base_reward))\n",
    "        self.base_reward = base_reward\n",
    "        self.attack_strength = attack_strength\n",
    "        self.pull_counts = np.zeros(num_arms)\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        reward = super().pull_arm(arm)  # Get reward before modifying probabilities\n",
    "        self.pull_counts[arm] += 1\n",
    "        \n",
    "        # Reduce the reward probability of the pulled arm\n",
    "        self.reward_means[arm] = max(0, self.reward_means[arm] - self.attack_strength)\n",
    "        \n",
    "        # Gradually restore other arms' probabilities toward base reward\n",
    "        for a in range(self.num_arms):\n",
    "            if a != arm:\n",
    "                self.reward_means[a] = min(self.base_reward, self.reward_means[a] + self.attack_strength/(self.num_arms-1))\n",
    "        \n",
    "        return reward\n",
    "\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        \"\"\"\n",
    "        Upper Confidence Bound (UCB) algorithm for multi-armed bandits.\n",
    "        :param num_arms: Number of arms\n",
    "        :param c: Exploration parameter\n",
    "        \"\"\"\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)  # Number of times each arm was selected\n",
    "        self.values = np.zeros(num_arms)  # Estimated values of each arm\n",
    "        self.t = 1  # Time step\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\"\n",
    "        Selects the arm using the UCB formula, ensuring unselected arms are picked first.\n",
    "        :return: Index of the selected arm\n",
    "        \"\"\"\n",
    "        ucb_values = np.where(\n",
    "            self.counts == 0,  # If arm has never been selected, assign infinity\n",
    "            np.inf,\n",
    "            self.values + self.c * np.sqrt(np.log(self.t) / self.counts)\n",
    "        )\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\"\n",
    "        Updates the estimated value of the selected arm.\n",
    "        :param arm: The selected arm\n",
    "        :param reward: The received reward\n",
    "        \"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "class EXP3:\n",
    "    def __init__(self, num_arms, gamma):\n",
    "        \"\"\"\n",
    "        Exponential-weighted Exploration and Exploitation (EXP3) algorithm.\n",
    "        :param num_arms: Number of arms\n",
    "        :param gamma: Exploration parameter\n",
    "        \"\"\"\n",
    "        self.num_arms = num_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\"\n",
    "        Selects an arm based on probability distribution derived from weights.\n",
    "        :return: Index of the selected arm\n",
    "        \"\"\"\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + self.gamma / self.num_arms\n",
    "        return np.random.choice(self.num_arms, p=probabilities)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\"\n",
    "        Updates the weights of the selected arm.\n",
    "        :param arm: The selected arm\n",
    "        :param reward: The received reward\n",
    "        \"\"\"\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + self.gamma / self.num_arms\n",
    "        estimated_reward = reward / probabilities[arm]\n",
    "        self.weights[arm] *= np.exp(self.gamma * estimated_reward / self.num_arms)\n",
    "        self.weights /= np.sum(self.weights)  # Normalize weights to prevent overflow\n",
    "        self.t += 1\n",
    "\n",
    "def run_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    \"\"\"\n",
    "    Runs the bandit experiment for a given algorithm and environment.\n",
    "    :param bandit_class: The bandit algorithm class (UCB or EXP3)\n",
    "    :param bandit_params: Parameters for the bandit algorithm\n",
    "    :param env: The environment (static or adversarial)\n",
    "    :param num_steps: Number of time steps\n",
    "    :return: Cumulative regret over time\n",
    "    \"\"\"\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    regrets = []\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        current_optimal = max(env.reward_means)\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        regret = current_optimal - reward\n",
    "        regrets.append(regret)\n",
    "        bandit.update(arm, reward)\n",
    "    \n",
    "    return np.cumsum(regrets)\n",
    "\n",
    "# Experimental setup\n",
    "num_arms = 10\n",
    "num_steps = 10000\n",
    "\n",
    "# Create environments\n",
    "static_env = BanditEnvironment(num_arms, np.full(num_arms, 0.7))\n",
    "adv_env = AdversarialBanditEnvironment(num_arms, base_reward=0.7, attack_strength=0.02)\n",
    "\n",
    "# Run experiments and plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Static Environment\n",
    "plt.subplot(1, 2, 1)\n",
    "ucb_static = run_experiment(UCB, {'num_arms': num_arms, 'c': 2}, static_env, num_steps)\n",
    "exp3_static = run_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, static_env, num_steps)\n",
    "plt.plot(ucb_static, label='UCB (Static)')\n",
    "plt.plot(exp3_static, label='EXP3 (Static)')\n",
    "plt.title(\"Static Environment (All arms p=0.7)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "\n",
    "# Adversarial Environment\n",
    "plt.subplot(1, 2, 2)\n",
    "ucb_adv = run_experiment(UCB, {'num_arms': num_arms, 'c': 2}, adv_env, num_steps)\n",
    "exp3_adv = run_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, adv_env, num_steps)\n",
    "plt.plot(ucb_adv, label='UCB (Adversarial)')\n",
    "plt.plot(exp3_adv, label='EXP3 (Adversarial)')\n",
    "plt.title(\"Adversarial Environment\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
