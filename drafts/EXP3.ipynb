{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Skills Report: Analysis of EXP3 in OTC Market Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Case Study – Analysis of EXP3 in OTC Market Pricing (EXP1 Experiment)\n",
    "\n",
    "### 4.1 Introduction and Background\n",
    "In over-the-counter (OTC) financial markets, liquidity providers (LPs) face significant challenges due to limited and asymmetric information. LPs typically do not know how many competitors are quoting prices, cannot observe rival quotes, and only receive information about their own trades. As a result, optimizing pricing strategies in such an environment is non-trivial.\n",
    "\n",
    "The EXP1 experiment, as presented in the study *“AI Driven Liquidity Provision in OTC Financial Markets”*, investigates whether model-free reinforcement learning algorithms—specifically the EXP3 multi-armed bandit algorithm—can successfully guide LPs in setting optimal spreads in such an opaque market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Market Structure and Mathematical Formulation\n",
    "#### Liquidity Provider Quotes\n",
    "Each LP posts a symmetric bid-ask quote around their own mid-price estimate of the true asset value:\n",
    "$$b^{(i)}_t = p^{(i)}_t - \\frac{s_i}{2}, \\quad a^{(i)}_t = p^{(i)}_t + \\frac{s_i}{2}$$\n",
    "\n",
    "where $s_i$ is the spread and $p^{(i)}_t = p^*_t + m^{(i)}_t$ is the mid-price estimate.\n",
    "\n",
    "#### Trader Behavior\n",
    "Trader's estimate and reservation prices are defined similarly, with rewards for LPs only if selected for a trade.\n",
    "\n",
    "#### Reward Function\n",
    "The LP’s reward is:\n",
    "$$\\pi^{(i)}_t = \\frac{1}{2}s_i \\cdot |D^{(i)}_t| + m^{(i)}_t \\cdot D^{(i)}_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 EXP3 Algorithm Deployment\n",
    "To determine optimal spreads under incomplete information, each LP employs the EXP3 bandit algorithm:\n",
    "```python\n",
    "# Simplified EXP3 Algorithm\n",
    "Initialize: weights = [1] * K, gamma = 0.1\n",
    "for t in range(T):\n",
    "    probabilities = [(1 - gamma) * (w / sum(weights)) + gamma / K for w in weights]\n",
    "    action = choose_action(probabilities)\n",
    "    reward = observe_reward(action)\n",
    "    weights[action] *= exp(gamma * reward / (K * probabilities[action]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Simulation Results and Visualization\n",
    "#### Figure 1A: LP1 Expected Value Surface\n",
    "A simulated plot of expected value based on spread combinations of LP1 and LP2 shows how LP1 adjusts its strategy in response to LP2’s behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Simulation Results and Visualization\n",
    "#### Figure 1A: LP1 Expected Value Surface\n",
    "A simulated plot of expected value based on spread combinations of LP1 and LP2 shows how LP1 adjusts its strategy in response to LP2’s behavior."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
