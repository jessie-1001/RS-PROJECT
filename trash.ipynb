{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes below are usless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original assignment 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DynamicPricingEnv:\n",
    "    def __init__(self, prices, change_points, reward_probabilities):\n",
    "        self.prices = prices\n",
    "        self.change_points = change_points\n",
    "        self.reward_probabilities = reward_probabilities\n",
    "        self.current_phase = 0\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, price_index):\n",
    "        if self.t in self.change_points:\n",
    "            self.current_phase = min(self.current_phase + 1, len(self.reward_probabilities) - 1)\n",
    "        self.t += 1\n",
    "        return np.random.rand() < self.reward_probabilities[self.current_phase][price_index]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_phase = 0\n",
    "        self.t = 0\n",
    "\n",
    "class DiscountedUCB:\n",
    "    def __init__(self, n_arms, gamma=0.99):\n",
    "        self.n_arms = n_arms\n",
    "        self.gamma = gamma\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        # TODO: Implement the arm selection logic for Discounted UCB\n",
    "        return np.random.randint(self.n_arms)  # Temporary fallback to avoid error\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        # TODO: Implement the update rule for Discounted UCB\n",
    "        pass\n",
    "\n",
    "class SlidingWindowUCB:\n",
    "    def __init__(self, n_arms, window_size=50):   # Default window size set to 50\n",
    "        self.n_arms = n_arms\n",
    "        self.window_size = window_size\n",
    "        self.rewards = [[] for _ in range(n_arms)]\n",
    "\n",
    "    def select_arm(self):\n",
    "        # TODO: Implement the arm selection logic for Sliding Window UCB\n",
    "        return np.random.randint(self.n_arms)  # Temporary fallback to avoid error\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        # TODO: Implement update rule for Sliding Window UCB\n",
    "        pass\n",
    "\n",
    "# Experiment Setup\n",
    "prices = [5, 10, 15, 20]\n",
    "change_points = [2000, 4000, 6000, 8000]\n",
    "reward_probabilities = [\n",
    "    [0.3, 0.5, 0.2, 0.1],  # Phase 1\n",
    "    [0.2, 0.6, 0.3, 0.15], # Phase 2\n",
    "    [0.1, 0.4, 0.5, 0.3],  # Phase 3\n",
    "    [0.25, 0.35, 0.3, 0.2], # Phase 4\n",
    "    [0.15, 0.5, 0.25, 0.4]  # Phase 5\n",
    "]\n",
    "\n",
    "env = DynamicPricingEnv(prices, change_points, reward_probabilities)\n",
    "d_ucb = DiscountedUCB(len(prices))\n",
    "sw_ucb = SlidingWindowUCB(len(prices))\n",
    "\n",
    "T = 10000  # Total time steps\n",
    "regrets_d = []\n",
    "regrets_sw = []\n",
    "\n",
    "for t in range(T):\n",
    "    # Discounted UCB\n",
    "    arm_d = d_ucb.select_arm()\n",
    "    reward_d = env.step(arm_d)\n",
    "    d_ucb.update(arm_d, reward_d)\n",
    "    regrets_d.append(max(reward_probabilities[env.current_phase]) - reward_d)\n",
    "\n",
    "    # Sliding Window UCB\n",
    "    arm_sw = sw_ucb.select_arm()\n",
    "    reward_sw = env.step(arm_sw)\n",
    "    sw_ucb.update(arm_sw, reward_sw)\n",
    "    regrets_sw.append(max(reward_probabilities[env.current_phase]) - reward_sw)\n",
    "\n",
    "# Plot Regrets\n",
    "plt.plot(np.cumsum(regrets_d), label='Discounted UCB')\n",
    "plt.plot(np.cumsum(regrets_sw), label='Sliding Window UCB')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of Discounted UCB and Sliding Window UCB in Dynamic Pricing\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT assignment 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DynamicPricingEnv:\n",
    "    def __init__(self, prices, change_points, reward_probabilities):\n",
    "        self.prices = prices\n",
    "        self.change_points = change_points\n",
    "        self.reward_probabilities = reward_probabilities\n",
    "        self.current_phase = 0\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, price_index):\n",
    "        if self.t in self.change_points:\n",
    "            self.current_phase = min(self.current_phase + 1, len(self.reward_probabilities) - 1)\n",
    "        self.t += 1\n",
    "        return np.random.rand() < self.reward_probabilities[self.current_phase][price_index]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_phase = 0\n",
    "        self.t = 0\n",
    "\n",
    "class DiscountedUCB:\n",
    "    def __init__(self, n_arms, gamma=0.99):\n",
    "        self.n_arms = n_arms\n",
    "        self.gamma = gamma\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        total_counts = np.sum(self.counts)\n",
    "        if total_counts == 0:\n",
    "            return np.random.randint(self.n_arms)\n",
    "\n",
    "        ucb_values = self.values + np.sqrt((2 * np.log(total_counts)) / (self.counts + 1e-6))\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts *= self.gamma\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] = ((1 - self.gamma) * self.values[arm]) + (self.gamma * reward)\n",
    "\n",
    "class SlidingWindowUCB:\n",
    "    def __init__(self, n_arms, window_size=50):\n",
    "        self.n_arms = n_arms\n",
    "        self.window_size = window_size\n",
    "        self.rewards = [[] for _ in range(n_arms)]\n",
    "\n",
    "    def select_arm(self):\n",
    "        ucb_values = []\n",
    "        for i in range(self.n_arms):\n",
    "            if len(self.rewards[i]) == 0:\n",
    "                ucb_values.append(float('inf'))\n",
    "            else:\n",
    "                mean_reward = np.mean(self.rewards[i])\n",
    "                confidence = np.sqrt((2 * np.log(sum(len(r) for r in self.rewards) + 1)) / len(self.rewards[i]))\n",
    "                ucb_values.append(mean_reward + confidence)\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.rewards[arm].append(reward)\n",
    "        if len(self.rewards[arm]) > self.window_size:\n",
    "            self.rewards[arm].pop(0)\n",
    "\n",
    "# Experiment Setup\n",
    "prices = [5, 10, 15, 20]\n",
    "change_points = [2000, 4000, 6000, 8000]\n",
    "reward_probabilities = [\n",
    "    [0.3, 0.5, 0.2, 0.1],  # Phase 1\n",
    "    [0.2, 0.6, 0.3, 0.15], # Phase 2\n",
    "    [0.1, 0.4, 0.5, 0.3],  # Phase 3\n",
    "    [0.25, 0.35, 0.3, 0.2], # Phase 4\n",
    "    [0.15, 0.5, 0.25, 0.4]  # Phase 5\n",
    "]\n",
    "\n",
    "env = DynamicPricingEnv(prices, change_points, reward_probabilities)\n",
    "d_ucb = DiscountedUCB(len(prices))\n",
    "sw_ucb = SlidingWindowUCB(len(prices))\n",
    "\n",
    "T = 10000  # Total time steps\n",
    "regrets_d = []\n",
    "regrets_sw = []\n",
    "\n",
    "for t in range(T):\n",
    "    # Discounted UCB\n",
    "    arm_d = d_ucb.select_arm()\n",
    "    reward_d = env.step(arm_d)\n",
    "    d_ucb.update(arm_d, reward_d)\n",
    "    regrets_d.append(max(reward_probabilities[env.current_phase]) - reward_d)\n",
    "\n",
    "    # Sliding Window UCB\n",
    "    arm_sw = sw_ucb.select_arm()\n",
    "    reward_sw = env.step(arm_sw)\n",
    "    sw_ucb.update(arm_sw, reward_sw)\n",
    "    regrets_sw.append(max(reward_probabilities[env.current_phase]) - reward_sw)\n",
    "\n",
    "# Plot Regrets\n",
    "plt.plot(np.cumsum(regrets_d), label='Discounted UCB')\n",
    "plt.plot(np.cumsum(regrets_sw), label='Sliding Window UCB')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of Discounted UCB and Sliding Window UCB in Dynamic Pricing\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original assignment 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bandit Environment\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = reward_means\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.reward_means[arm])  # Bernoulli rewards\n",
    "\n",
    "# ==========================\n",
    "# Fixed Exploration Then Exploitation (TODO: Implement switching strategy)\n",
    "# ==========================\n",
    "class FixedExplorationThenGreedy:\n",
    "    def __init__(self, num_arms, exploration_steps):\n",
    "        self.num_arms = num_arms\n",
    "        self.exploration_steps = exploration_steps\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement fixed exploration for N steps, then greedy \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "# ==========================\n",
    "# Epsilon-Greedy Algorithm (TODO: Complete update function)\n",
    "# ==========================\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, num_arms, epsilon):\n",
    "        self.num_arms = num_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement selection rule \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement incremental mean update \"\"\"\n",
    "        pass\n",
    "\n",
    "# ==========================\n",
    "# Epsilon-Greedy with Decaying Exploration (TODO: Complete decay schedule and compare schedules)\n",
    "# ==========================\n",
    "class EpsilonGreedyDecaying:\n",
    "    def __init__(self, num_arms, epsilon_schedule):\n",
    "        self.num_arms = num_arms\n",
    "        self.epsilon_schedule = epsilon_schedule  # Function for epsilon_t\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement selection rule \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement update rule including epsilon decay \"\"\"\n",
    "        pass\n",
    "\n",
    "# ==========================\n",
    "# UCB Algorithm (TODO: Complete selection function)\n",
    "# ==========================\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement UCB selection rule \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "# ==========================\n",
    "# Thompson Sampling Algorithm (TODO: Implement Thompson Sampling)\n",
    "# ==========================\n",
    "class ThompsonSampling:\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.successes = np.zeros(num_arms)\n",
    "        self.failures = np.zeros(num_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement Thompson Sampling selection rule \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement update rule for Beta distribution \"\"\"\n",
    "        pass\n",
    "\n",
    "# ==========================\n",
    "# Experiment Runner\n",
    "# ==========================\n",
    "def run_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    regrets = []\n",
    "    optimal_reward = max(env.reward_means)\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        bandit.update(arm, reward)\n",
    "        regret = optimal_reward - reward\n",
    "        regrets.append(regret)\n",
    "\n",
    "    return np.cumsum(regrets)\n",
    "\n",
    "# ==========================\n",
    "# Running Experiments\n",
    "# ==========================\n",
    "num_arms = 10\n",
    "reward_means = np.linspace(0, 1, num_arms)  # Linearly spaced rewards\n",
    "env = BanditEnvironment(num_arms, reward_means)\n",
    "num_steps = 10000\n",
    "\n",
    "# Define epsilon schedule\n",
    "def epsilon_schedule(t):\n",
    "    return 1 / (t + 1)\n",
    "\n",
    "# Plot setup\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Run and plot Fixed Exploration Then Exploitation\n",
    "fixed_exploration_regret = run_experiment(FixedExplorationThenGreedy, {'num_arms': num_arms, 'exploration_steps': 100}, env, num_steps)\n",
    "plt.plot(fixed_exploration_regret, label='Fixed Exploration')\n",
    "\n",
    "# Run and plot ε-Greedy\n",
    "epsilon_greedy_regret = run_experiment(EpsilonGreedy, {'num_arms': num_arms, 'epsilon': 0.1}, env, num_steps)\n",
    "plt.plot(epsilon_greedy_regret, label='Epsilon-Greedy')\n",
    "\n",
    "# Run and plot Decaying ε-Greedy\n",
    "decaying_epsilon_greedy_regret = run_experiment(EpsilonGreedyDecaying, {'num_arms': num_arms, 'epsilon_schedule': epsilon_schedule}, env, num_steps)\n",
    "plt.plot(decaying_epsilon_greedy_regret, label='Decaying Epsilon-Greedy')\n",
    "\n",
    "# Run and plot UCB\n",
    "ucb_regret = run_experiment(UCB, {'num_arms': num_arms, 'c': 4}, env, num_steps)\n",
    "plt.plot(ucb_regret, label='UCB')\n",
    "\n",
    "# Run and plot Thompson Sampling\n",
    "thompson_regret = run_experiment(ThompsonSampling, {'num_arms': num_arms}, env, num_steps)\n",
    "plt.plot(thompson_regret, label='Thompson Sampling')\n",
    "\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "plt.title(\"Bandit Algorithm Performance\")\n",
    "plt.show()\n",
    "\n",
    "# ==========================\n",
    "# Instructions for Students\n",
    "# ==========================\n",
    "print(\"TODO: Complete the missing functions for Fixed-Exploration-Greedy, Epsilon-Greedy, Decaying Epsilon-Greedy, UCB, and Thompson Sampling.\")\n",
    "print(\"TODO: Implement and compare different epsilon schedules (e.g., 1/t, 1/sqrt(t), log(t)/t). Discuss the impact on exploration and cumulative regret in your report.\")\n",
    "print(\"TODO: Answer the questions in the assignment and conduct the necessary experiments to answer them.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT assignment 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bandit Environment\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = reward_means\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.reward_means[arm])  # Bernoulli rewards\n",
    "\n",
    "# ==========================\n",
    "# Fixed Exploration Then Exploitation (TODO: Implement switching strategy)\n",
    "# ==========================\n",
    "class FixedExplorationThenGreedy:\n",
    "    def __init__(self, num_arms, exploration_steps):\n",
    "        self.num_arms = num_arms\n",
    "        self.exploration_steps = exploration_steps\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement fixed exploration for N steps, then greedy \"\"\"\n",
    "        if self.t <= self.exploration_steps:\n",
    "            return np.random.randint(self.num_arms)  # Random exploration\n",
    "        return np.argmax(self.values)  # Greedy exploitation\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "# ==========================\n",
    "# Epsilon-Greedy Algorithm (TODO: Complete update function)\n",
    "# ==========================\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, num_arms, epsilon):\n",
    "        self.num_arms = num_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement selection rule \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.num_arms)  # Explore\n",
    "        return np.argmax(self.values)  # Exploit\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement incremental mean update \"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "\n",
    "# ==========================\n",
    "# Epsilon-Greedy with Decaying Exploration (TODO: Complete decay schedule and compare schedules)\n",
    "# ==========================\n",
    "class EpsilonGreedyDecaying:\n",
    "    def __init__(self, num_arms, epsilon_schedule):\n",
    "        self.num_arms = num_arms\n",
    "        self.epsilon_schedule = epsilon_schedule  # Function for epsilon_t\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement selection rule \"\"\"\n",
    "        epsilon_t = self.epsilon_schedule(self.t)\n",
    "        if np.random.rand() < epsilon_t:\n",
    "            return np.random.randint(self.num_arms)  # Explore\n",
    "        return np.argmax(self.values)  # Exploit\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement update rule including epsilon decay \"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "# ==========================\n",
    "# UCB Algorithm (TODO: Complete selection function)\n",
    "# ==========================\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" Implement UCB selection rule \"\"\"\n",
    "        total_counts = np.sum(self.counts)\n",
    "        if total_counts < self.num_arms:  # Ensure each arm is tried at least once\n",
    "            return int(total_counts)  # Explicitly cast to integer\n",
    "        ucb_values = self.values + self.c * np.sqrt(np.log(total_counts) / (self.counts + 1e-6))\n",
    "        return int(np.argmax(ucb_values))  # Ensure output is an integer\n",
    "\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "# ==========================\n",
    "# Thompson Sampling Algorithm (TODO: Implement Thompson Sampling)\n",
    "# ==========================\n",
    "class ThompsonSampling:\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.successes = np.zeros(num_arms)\n",
    "        self.failures = np.zeros(num_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement Thompson Sampling selection rule \"\"\"\n",
    "        samples = [np.random.beta(self.successes[i] + 1, self.failures[i] + 1) for i in range(self.num_arms)]\n",
    "        return np.argmax(samples)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement update rule for Beta distribution \"\"\"\n",
    "        if reward == 1:\n",
    "            self.successes[arm] += 1\n",
    "        else:\n",
    "            self.failures[arm] += 1\n",
    "\n",
    "# ==========================\n",
    "# Experiment Runner\n",
    "# ==========================\n",
    "def run_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    regrets = []\n",
    "    optimal_reward = max(env.reward_means)\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        bandit.update(arm, reward)\n",
    "        regret = optimal_reward - reward\n",
    "        regrets.append(regret)\n",
    "\n",
    "    return np.cumsum(regrets)\n",
    "\n",
    "# ==========================\n",
    "# Running Experiments\n",
    "# ==========================\n",
    "num_arms = 10\n",
    "reward_means = np.linspace(0, 1, num_arms)  # Linearly spaced rewards\n",
    "env = BanditEnvironment(num_arms, reward_means)\n",
    "num_steps = 10000\n",
    "\n",
    "# Define epsilon schedule\n",
    "def epsilon_schedule(t):\n",
    "    return 1 / (t + 1)\n",
    "\n",
    "# Plot setup\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Run and plot Fixed Exploration Then Exploitation\n",
    "fixed_exploration_regret = run_experiment(FixedExplorationThenGreedy, {'num_arms': num_arms, 'exploration_steps': 100}, env, num_steps)\n",
    "plt.plot(fixed_exploration_regret, label='Fixed Exploration')\n",
    "\n",
    "# Run and plot ε-Greedy\n",
    "epsilon_greedy_regret = run_experiment(EpsilonGreedy, {'num_arms': num_arms, 'epsilon': 0.1}, env, num_steps)\n",
    "plt.plot(epsilon_greedy_regret, label='Epsilon-Greedy')\n",
    "\n",
    "# Run and plot Decaying ε-Greedy\n",
    "decaying_epsilon_greedy_regret = run_experiment(EpsilonGreedyDecaying, {'num_arms': num_arms, 'epsilon_schedule': epsilon_schedule}, env, num_steps)\n",
    "plt.plot(decaying_epsilon_greedy_regret, label='Decaying Epsilon-Greedy')\n",
    "\n",
    "# Run and plot UCB\n",
    "ucb_regret = run_experiment(UCB, {'num_arms': num_arms, 'c': 4}, env, num_steps)\n",
    "plt.plot(ucb_regret, label='UCB')\n",
    "\n",
    "# Run and plot Thompson Sampling\n",
    "thompson_regret = run_experiment(ThompsonSampling, {'num_arms': num_arms}, env, num_steps)\n",
    "plt.plot(thompson_regret, label='Thompson Sampling')\n",
    "\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "plt.title(\"Bandit Algorithm Performance\")\n",
    "plt.show()\n",
    "\n",
    "# ==========================\n",
    "# Instructions for Students\n",
    "# ==========================\n",
    "print(\"TODO: Complete the missing functions for Fixed-Exploration-Greedy, Epsilon-Greedy, Decaying Epsilon-Greedy, UCB, and Thompson Sampling.\")\n",
    "print(\"TODO: Implement and compare different epsilon schedules (e.g., 1/t, 1/sqrt(t), log(t)/t). Discuss the impact on exploration and cumulative regret in your report.\")\n",
    "print(\"TODO: Answer the questions in the assignment and conduct the necessary experiments to answer them.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = reward_means\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.reward_means[arm])\n",
    "\n",
    "class DynamicBanditEnvironment(BanditEnvironment):\n",
    "    def __init__(self, num_arms, reward_means, volatility=0.001):\n",
    "        super().__init__(num_arms, reward_means)\n",
    "        self.volatility = volatility\n",
    "        self.original_means = reward_means.copy()\n",
    "        \n",
    "    def pull_arm(self, arm):\n",
    "        # Simulate dynamic environment with mean reversion\n",
    "        self.reward_means += np.random.normal(0, self.volatility, self.num_arms)\n",
    "        self.reward_means = np.clip(self.reward_means, 0, 1)\n",
    "        return np.random.binomial(1, self.reward_means[arm])\n",
    "\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        #for arm in range(self.num_arms):\n",
    "        #    if self.counts[arm] == 0:\n",
    "        #        return arm\n",
    "\n",
    "        if np.any(self.counts == 0):  # Ensure each arm is picked once\n",
    "            untried_arms = np.where(self.counts == 0)[0]\n",
    "            return np.random.choice(untried_arms)  # Randomize initial selections\n",
    "\n",
    "        ucb_values = self.values + self.c * np.sqrt(np.log(self.t) / self.counts)\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "class EXP3:\n",
    "    def __init__(self, num_arms, gamma):\n",
    "        self.num_arms = num_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + \\\n",
    "                       self.gamma / self.num_arms\n",
    "        return np.random.choice(self.num_arms, p=probabilities)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + \\\n",
    "                       self.gamma / self.num_arms\n",
    "        estimated_reward = reward / probabilities[arm]\n",
    "        self.weights[arm] *= np.exp(self.gamma * estimated_reward / self.num_arms)\n",
    "        self.t += 1\n",
    "\n",
    "def run_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    regrets = []\n",
    "    optimal_reward = max(env.reward_means if isinstance(env, BanditEnvironment) else env.original_means)\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        bandit.update(arm, reward)\n",
    "\n",
    "        regret = optimal_reward - reward\n",
    "        regrets.append(regret)\n",
    "\n",
    "    return np.cumsum(regrets)\n",
    "\n",
    "def run_multi_agent_experiment(bandit_classes, bandit_params_list, env, num_steps):\n",
    "    bandits = [cls(**params) for cls, params in zip(bandit_classes, bandit_params_list)]\n",
    "    regrets = {i: [] for i in range(len(bandits))}\n",
    "    optimal_reward = max(env.original_means)\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        for i, bandit in enumerate(bandits):\n",
    "            arm = bandit.select_arm()\n",
    "            reward = env.pull_arm(arm)\n",
    "            bandit.update(arm, reward)\n",
    "            regret = optimal_reward - reward\n",
    "            regrets[i].append(regret)\n",
    "\n",
    "    return {k: np.cumsum(v) for k, v in regrets.items()}\n",
    "\n",
    "# Experimental setup\n",
    "num_arms = 10\n",
    "base_reward_means = np.linspace(0, 1, num_arms)\n",
    "num_steps = 10000\n",
    "\n",
    "# Single-Agent Static Environment\n",
    "static_env = BanditEnvironment(num_arms, base_reward_means.copy())\n",
    "\n",
    "# Multi-Agent Dynamic Environment\n",
    "dynamic_env = DynamicBanditEnvironment(num_arms, base_reward_means.copy(), volatility=0.002)\n",
    "\n",
    "# Run experiments\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Single-Agent Experiments\n",
    "plt.subplot(1, 2, 1)\n",
    "ucb_static_regret = run_experiment(UCB, {'num_arms': num_arms, 'c': 2}, static_env, num_steps)\n",
    "exp3_static_regret = run_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, static_env, num_steps)\n",
    "\n",
    "plt.plot(ucb_static_regret, label='UCB (Static)')\n",
    "plt.plot(exp3_static_regret, label='EXP3 (Static)')\n",
    "plt.title(\"Single-Agent Static Environment\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Multi-Agent Dynamic Environment\n",
    "plt.subplot(1, 2, 2)\n",
    "dynamic_results = run_multi_agent_experiment(\n",
    "    [UCB, EXP3, UCB, EXP3],\n",
    "    [{'num_arms': num_arms, 'c': 2},\n",
    "     {'num_arms': num_arms, 'gamma': 0.1},\n",
    "     {'num_arms': num_arms, 'c': 2},\n",
    "     {'num_arms': num_arms, 'gamma': 0.1}],\n",
    "    dynamic_env,\n",
    "    num_steps\n",
    ")\n",
    "\n",
    "# Average results for each algorithm type\n",
    "ucb_dynamic_regret = (dynamic_results[0] + dynamic_results[2]) / 2\n",
    "exp3_dynamic_regret = (dynamic_results[1] + dynamic_results[3]) / 2\n",
    "\n",
    "plt.plot(ucb_dynamic_regret, label='UCB (Dynamic)')\n",
    "plt.plot(exp3_dynamic_regret, label='EXP3 (Dynamic)')\n",
    "plt.title(\"Multi-Agent Dynamic Environment\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TrackingBanditEnvironment(BanditEnvironment):\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        super().__init__(num_arms, reward_means)\n",
    "        self.optimal_arm = np.argmax(reward_means)\n",
    "        \n",
    "# Modified experiment function to track arm probabilities\n",
    "def run_tracking_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    arm_selections = np.zeros((num_steps, env.num_arms))\n",
    "    \n",
    "    for t in range(num_steps):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        bandit.update(arm, reward)\n",
    "        arm_selections[t, arm] = 1\n",
    "    \n",
    "    # Calculate cumulative probabilities\n",
    "    cumulative_counts = np.cumsum(arm_selections, axis=0)\n",
    "    total_counts = np.arange(1, num_steps+1)[:, np.newaxis]\n",
    "    return cumulative_counts / total_counts\n",
    "\n",
    "# Set up environment with clear optimal arm (mirroring paper's s values)\n",
    "num_arms = 5\n",
    "reward_means = np.array([0.1, 0.3, 0.5, 0.7, 0.9])  # Explicit s values (spreads)\n",
    "static_env = TrackingBanditEnvironment(num_arms, reward_means)\n",
    "num_steps = 10000\n",
    "\n",
    "# Run tracking experiments\n",
    "ucb_probs = run_tracking_experiment(UCB, {'num_arms': num_arms, 'c': 2}, static_env, num_steps)\n",
    "exp3_probs = run_tracking_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, static_env, num_steps)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# UCB Arm Selection Probabilities\n",
    "plt.subplot(1, 2, 1)\n",
    "for arm in range(num_arms):\n",
    "    plt.plot(ucb_probs[:, arm], label=f'Arm {arm+1} (s={reward_means[arm]:.1f})', alpha=0.8)\n",
    "plt.axhline(y=0.9, color='black', linestyle='--', alpha=0.5)\n",
    "plt.title('UCB: Arm Selection Probability Over Time')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability of Arm Draw')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# EXP3 Arm Selection Probabilities\n",
    "plt.subplot(1, 2, 2)\n",
    "for arm in range(num_arms):\n",
    "    plt.plot(exp3_probs[:, arm], label=f'Arm {arm+1} (s={reward_means[arm]:.1f})', alpha=0.8)\n",
    "plt.axhline(y=0.9, color='black', linestyle='--', alpha=0.5)\n",
    "plt.title('EXP3: Arm Selection Probability Over Time')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability of Arm Draw')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = reward_means\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.reward_means[arm])\n",
    "\n",
    "class FixedCompetitorEnvironment(BanditEnvironment):\n",
    "    def __init__(self, num_arms, reward_means, competitor_arm):\n",
    "        super().__init__(num_arms, reward_means)\n",
    "        self.competitor_arm = competitor_arm\n",
    "        self.competitor_value = reward_means[competitor_arm]\n",
    "        \n",
    "    def pull_arm(self, arm):\n",
    "        # Reward is probability of winning against competitor\n",
    "        return np.random.binomial(1, self.reward_means[arm] > self.competitor_value)\n",
    "\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        if np.any(self.counts == 0):\n",
    "            return np.random.choice(np.where(self.counts == 0)[0])\n",
    "        ucb_values = self.values + self.c * np.sqrt(np.log(self.t) / (self.counts + 1e-6))\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "class EXP3:\n",
    "    def __init__(self, num_arms, gamma):\n",
    "        self.num_arms = num_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(num_arms)\n",
    "        \n",
    "    def select_arm(self):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + self.gamma / self.num_arms\n",
    "        return np.random.choice(self.num_arms, p=probabilities)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + self.gamma / self.num_arms\n",
    "        estimated_reward = reward / probabilities[arm]\n",
    "        self.weights[arm] *= np.exp(self.gamma * estimated_reward / self.num_arms)\n",
    "\n",
    "\n",
    "def run_tracking_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    arm_selections = np.zeros((num_steps, env.num_arms))\n",
    "    \n",
    "    for t in range(num_steps):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        bandit.update(arm, reward)\n",
    "        arm_selections[t, arm] = 1\n",
    "    \n",
    "    return np.cumsum(arm_selections, axis=0) / np.arange(1, num_steps+1)[:, np.newaxis]\n",
    "\n",
    "# Experimental setup\n",
    "num_arms = 5\n",
    "reward_means = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "competitor_arm = 3  # Competitor selects arm index 3 (s = 0.3)\n",
    "num_steps = 5000\n",
    "\n",
    "# Create environment with fixed competitor\n",
    "env = FixedCompetitorEnvironment(num_arms, reward_means, competitor_arm)\n",
    "\n",
    "# Run experiments\n",
    "ucb_probs = run_tracking_experiment(UCB, {'num_arms': num_arms, 'c': 2}, env, num_steps)\n",
    "exp3_probs = run_tracking_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, env, num_steps)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# UCB Arm Selection Probabilities\n",
    "plt.subplot(1, 2, 1)\n",
    "for arm in range(num_arms):\n",
    "    plt.plot(ucb_probs[:, arm], label=f'Arm {arm+1} (p={reward_means[arm]:.1f})', alpha=0.8)\n",
    "plt.title('UCB: Probability of Arm Selection Over Time')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# EXP3 Arm Selection Probabilities\n",
    "plt.subplot(1, 2, 2)\n",
    "for arm in range(num_arms):\n",
    "    plt.plot(exp3_probs[:, arm], label=f'Arm {arm+1} (p={reward_means[arm]:.1f})', alpha=0.8)\n",
    "plt.title('EXP3: Probability of Arm Selection Over Time')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "class CompetitiveBanditEnvironment:\n",
    "    def __init__(self, spreads, competitor_spread, s_T=1.0, ω=0.15, ρ=0.5):\n",
    "        \"\"\"\n",
    "        Parameters from paper:\n",
    "        - s_T: Trader's reservation spread\n",
    "        - ω: Measurement error standard deviation\n",
    "        - ρ: Correlation between LPs' measurement errors\n",
    "        \"\"\"\n",
    "        self.spreads = spreads\n",
    "        self.competitor_spread = competitor_spread\n",
    "        self.s_T = s_T\n",
    "        self.ω = ω\n",
    "        self.ρ = ρ\n",
    "        \n",
    "        # Precompute terms from paper's equation (11)\n",
    "        self.ω_T = ω * np.sqrt(ρ + (1-ρ)/2)  # For N=2 competitors\n",
    "        \n",
    "    def get_win_probability(self, s_i):\n",
    "        \"\"\"Calculate probability of winning against competitor (equation 11)\"\"\"\n",
    "        # Simplified version of the paper's integral formulation\n",
    "        # Using probit approximation for the cumulative normal\n",
    "        z = (self.s_T - s_i) / (2 * self.ω_T)\n",
    "        return norm.cdf(z) * norm.cdf((self.competitor_spread - s_i)/(2*self.ω*np.sqrt(1-self.ρ)))\n",
    "    \n",
    "    def get_reward(self, arm):\n",
    "        \"\"\"Full reward calculation from paper's equation (7)\"\"\"\n",
    "        s_i = self.spreads[arm]\n",
    "        win_prob = self.get_win_probability(s_i)\n",
    "        \n",
    "        # Expected reward components\n",
    "        spread_reward = s_i/2 * win_prob\n",
    "        adverse_selection = -self.ω * np.sqrt(1-self.ρ) * (1/np.sqrt(2*np.pi))  # Simplified μ_i^(1)\n",
    "        \n",
    "        # Add noise for exploration\n",
    "        noise = np.random.normal(0, 0.05)\n",
    "        return spread_reward + adverse_selection + noise\n",
    "\n",
    "class UCB:\n",
    "    def __init__(self, n_arms, c=1.5):\n",
    "        self.n_arms = n_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "        self.total_counts = 0\n",
    "        \n",
    "    def select_arm(self):\n",
    "        if self.total_counts < self.n_arms:\n",
    "            return self.total_counts  # Initial exploration\n",
    "        \n",
    "        ucb = self.values + self.c * np.sqrt(np.log(self.total_counts)/(self.counts + 1e-6))\n",
    "        return np.argmax(ucb)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.total_counts += 1\n",
    "        self.values[arm] += (reward - self.values[arm])/self.counts[arm]\n",
    "\n",
    "class EXP3:\n",
    "    def __init__(self, n_arms, gamma=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(n_arms)\n",
    "        self.probabilities = np.ones(n_arms)/n_arms\n",
    "        \n",
    "    def select_arm(self):\n",
    "        self.probabilities = (1-self.gamma)*self.weights/self.weights.sum() + self.gamma/self.n_arms\n",
    "        return np.random.choice(self.n_arms, p=self.probabilities)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        # Normalize reward to [-1,1] for stability\n",
    "        normalized_reward = (reward - 0.2) / 0.5  # Based on observed reward range\n",
    "        estimated_reward = normalized_reward / self.probabilities[arm]\n",
    "        self.weights[arm] *= np.exp(self.gamma * estimated_reward/self.n_arms)\n",
    "        self.weights = np.clip(self.weights, 1e-6, 1e6)\n",
    "\n",
    "# Experimental setup\n",
    "spreads = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "competitor_spread = 0.7\n",
    "env = CompetitiveBanditEnvironment(spreads=spreads,\n",
    "                                  competitor_spread=competitor_spread,\n",
    "                                  s_T=1.0,\n",
    "                                  ω=0.15,\n",
    "                                  ρ=0.5)\n",
    "\n",
    "# Run experiments\n",
    "n_steps = 10000\n",
    "n_runs = 20\n",
    "\n",
    "def run_experiment(algorithm, env):\n",
    "    history = np.zeros((n_steps, len(spreads)))\n",
    "    for t in range(n_steps):\n",
    "        arm = algorithm.select_arm()\n",
    "        reward = env.get_reward(arm)\n",
    "        algorithm.update(arm, reward)\n",
    "        \n",
    "        # Track empirical probabilities\n",
    "        if isinstance(algorithm, UCB):\n",
    "            probs = algorithm.counts / (algorithm.total_counts + 1e-6)\n",
    "        else:\n",
    "            probs = algorithm.probabilities\n",
    "        history[t] = probs\n",
    "    return history\n",
    "\n",
    "# Aggregate results\n",
    "ucb_probs = np.zeros((n_steps, len(spreads)))\n",
    "exp3_probs = np.zeros((n_steps, len(spreads)))\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    ucb = UCB(n_arms=len(spreads))\n",
    "    exp3 = EXP3(n_arms=len(spreads))\n",
    "    \n",
    "    ucb_probs += run_experiment(ucb, env)\n",
    "    exp3_probs += run_experiment(exp3, env)\n",
    "\n",
    "ucb_probs /= n_runs\n",
    "exp3_probs /= n_runs\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, s in enumerate(spreads):\n",
    "    plt.plot(ucb_probs[:, i], label=f's={s:.1f}')\n",
    "plt.title('UCB: Arm Selection Probabilities')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, s in enumerate(spreads):\n",
    "    plt.plot(exp3_probs[:, i], label=f's={s:.1f}')\n",
    "plt.title('EXP3: Arm Selection Probabilities')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import dblquad\n",
    "from functools import lru_cache\n",
    "\n",
    "class OTCMarketBandit:\n",
    "    \"\"\"\n",
    "    Bandit environment implementing the OTC market dynamics from the paper\n",
    "    Key differences from standard bandits:\n",
    "    1. Rewards depend on both spread selection and hidden competitor spreads\n",
    "    2. Non-stationary due to competitor reactions (simulated via fixed spread)\n",
    "    3. Reward includes adverse selection costs (Winner's Curse)\n",
    "    \"\"\"\n",
    "    def __init__(self, arms, competitor_spread=0.7, s_T=1.0, ω=0.15, ρ=0.5):\n",
    "        \"\"\"\n",
    "        :param arms: array of candidate spreads (arms)\n",
    "        :param competitor_spread: fixed spread of competitor (s_2 in paper)\n",
    "        :param s_T: trader's reservation spread\n",
    "        :param ω: volatility of price estimates\n",
    "        :param ρ: correlation between LPs' price estimates\n",
    "        \"\"\"\n",
    "        self.arms = arms\n",
    "        self.s_2 = competitor_spread\n",
    "        self.s_T = s_T\n",
    "        self.ω = ω\n",
    "        self.ρ = ρ\n",
    "        self.ω_T = ω * np.sqrt(ρ + (1-ρ)/2)  # Trader's estimation error\n",
    "        \n",
    "        # Precompute theoretical values for faster access\n",
    "        self.θ_values = np.array([self._compute_θ(s_i) for s_i in arms])\n",
    "        self.μ_values = np.array([self._compute_μ(s_i) for s_i in arms])\n",
    "        \n",
    "    @lru_cache(maxsize=None)\n",
    "    def _compute_θ(self, s_i):\n",
    "        \"\"\"Compute θ_i^(1) from equation (9)\"\"\"\n",
    "        def integrand(x, y):\n",
    "            z1 = (self.ω*np.sqrt(self.ρ)*x + self.ω*np.sqrt(1-self.ρ)*y)/self.ω_T + (self.s_T - s_i)/(2*self.ω_T)\n",
    "            z2 = y + (self.s_2 - s_i)/(2*self.ω*np.sqrt(1-self.ρ))\n",
    "            return norm.cdf(z1) * norm.cdf(z2) * norm.pdf(x) * norm.pdf(y)\n",
    "        return dblquad(integrand, -np.inf, np.inf, -np.inf, np.inf)[0]\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def _compute_μ(self, s_i):\n",
    "        \"\"\"Compute μ_i^(1) from equation (10)\"\"\"\n",
    "        def integrand(x, y):\n",
    "            z = y + np.sqrt(self.ρ/(1-self.ρ))*x\n",
    "            return z * self._compute_θ(s_i) * norm.pdf(x) * norm.pdf(y)\n",
    "        return dblquad(integrand, -np.inf, np.inf, -np.inf, np.inf)[0]\n",
    "\n",
    "    def get_reward(self, arm):\n",
    "        \"\"\"\n",
    "        Returns stochastic reward for selected arm (spread)\n",
    "        Implements equation (7) from the paper:\n",
    "        π_i = s_i/2 * θ_i^(1) - ω√(1-ρ) * μ_i^(1) + noise\n",
    "        \"\"\"\n",
    "        s_i = self.arms[arm]\n",
    "        θ = self.θ_values[arm]\n",
    "        μ = self.μ_values[arm]\n",
    "        deterministic = 0.5*s_i*θ - self.ω*np.sqrt(1-self.ρ)*μ\n",
    "        return deterministic + np.random.normal(0, 0.01)  # Add small noise\n",
    "\n",
    "class UCB1:\n",
    "    \"\"\"\n",
    "    Modified UCB1 algorithm adapted for OTC market dynamics\n",
    "    Key adjustments:\n",
    "    - Tuned exploration parameter for financial spreads\n",
    "    - Optimized for delayed feedback characteristics\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, c=2):\n",
    "        self.n_arms = n_arms\n",
    "        self.c = c  # Exploration parameter\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "        \n",
    "    def select_arm(self):\n",
    "        if np.any(self.counts == 0):\n",
    "            return np.argmax(self.counts == 0)  # Force initial exploration\n",
    "        ucb = self.values + self.c * np.sqrt(np.log(np.sum(self.counts)) / (self.counts + 1e-6))\n",
    "        return np.argmax(ucb)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "\n",
    "class EXP3:\n",
    "    \"\"\"\n",
    "    EXP3 implementation with financial market adaptations:\n",
    "    - Reward normalization for stability\n",
    "    - Optimized exploration rate for spread environments\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, gamma=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(n_arms)\n",
    "        self.probs = np.ones(n_arms)/n_arms\n",
    "        \n",
    "    def select_arm(self):\n",
    "        self.probs = (1-self.gamma)*self.weights/self.weights.sum() + self.gamma/self.n_arms\n",
    "        return np.random.choice(self.n_arms, p=self.probs)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        normalized_reward = (reward - 0.2) / 0.5  # Based on observed reward range\n",
    "        estimated = normalized_reward / self.probs[arm]\n",
    "        self.weights[arm] *= np.exp(self.gamma * estimated / self.n_arms)\n",
    "        self.weights = np.clip(self.weights, 1e-6, 1e6)  # Prevent overflow\n",
    "\n",
    "def run_experiment(bandit, algorithm, steps=5000):\n",
    "    \"\"\"Run bandit algorithm in environment\"\"\"\n",
    "    history = np.zeros((steps, len(bandit.arms)))\n",
    "    for t in range(steps):\n",
    "        arm = algorithm.select_arm()\n",
    "        reward = bandit.get_reward(arm)\n",
    "        algorithm.update(arm, reward)\n",
    "        \n",
    "        # Track empirical probabilities\n",
    "        if isinstance(algorithm, UCB1):\n",
    "            probs = algorithm.counts / (np.sum(algorithm.counts) + 1e-6)\n",
    "        else:\n",
    "            probs = algorithm.probs\n",
    "        history[t] = probs\n",
    "    return history\n",
    "\n",
    "# Parameters matching paper's baseline\n",
    "arms = np.array([0.1, 0.3, 0.5, 0.7, 0.9])  # Candidate spreads\n",
    "competitor_spread = 0.7  # Fixed competitor spread (s_2)\n",
    "steps = 5000\n",
    "trials = 20\n",
    "\n",
    "# Initialize environment and algorithms\n",
    "bandit = OTCMarketBandit(arms, competitor_spread)\n",
    "ucb = UCB1(len(arms))\n",
    "exp3 = EXP3(len(arms))\n",
    "\n",
    "# Run experiments\n",
    "ucb_results = np.zeros((steps, len(arms)))\n",
    "exp3_results = np.zeros((steps, len(arms)))\n",
    "\n",
    "for _ in range(trials):\n",
    "    ucb_results += run_experiment(bandit, UCB1(len(arms)), steps)\n",
    "    exp3_results += run_experiment(bandit, EXP3(len(arms)), steps)\n",
    "\n",
    "ucb_results /= trials\n",
    "exp3_results /= trials\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(len(arms)):\n",
    "    plt.plot(ucb_results[:, i], label=f's={arms[i]:.1f}')\n",
    "plt.title('UCB: Arm Selection Probability')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(len(arms)):\n",
    "    plt.plot(exp3_results[:, i], label=f's={arms[i]:.1f}')\n",
    "plt.title('EXP3: Arm Selection Probability')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Multi-Agent Stationary OTC Environment\n",
    "# ==========================\n",
    "class MultiAgentOTCEnvironment(OTCEnvironment):\n",
    "    def __init__(self, spreads, competitor_spread, alpha=0.3, penalty_mode='linear', adversarial_penalty=0.1):\n",
    "        super().__init__(spreads, competitor_spread, alpha)\n",
    "        self.penalty_mode = penalty_mode\n",
    "        self.adversarial_penalty = adversarial_penalty\n",
    "\n",
    "    def get_penalty_multiplier(self, count):\n",
    "        if self.penalty_mode == 'linear':\n",
    "            return 1.0 / (1.0 + self.adversarial_penalty * count)\n",
    "        elif self.penalty_mode == 'log':\n",
    "            return 1.0 / (1.0 + self.adversarial_penalty * np.log(1 + count))\n",
    "        elif self.penalty_mode == 'none':\n",
    "            return 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported penalty mode: {self.penalty_mode}\")\n",
    "\n",
    "    def pull_arms_multiagent(self, arm_selections):\n",
    "        arm_counts = np.bincount(arm_selections, minlength=len(self.spreads))\n",
    "        rewards = []\n",
    "        for arm in arm_selections:\n",
    "            s = self.spreads[arm]\n",
    "            prob = self.get_execution_prob(s)\n",
    "            win = np.random.binomial(1, prob)\n",
    "            penalty = self.get_penalty_multiplier(arm_counts[arm])\n",
    "            rewards.append(win * s * penalty)\n",
    "        return rewards\n",
    "\n",
    "# ==========================\n",
    "# Multi-Agent Experiment Runner\n",
    "# ==========================\n",
    "def run_multiagent_experiment(bandit_class, bandit_params, env, num_agents, num_steps):\n",
    "    agents = [bandit_class(**bandit_params) for _ in range(num_agents)]\n",
    "    expected_rewards = [s * env.get_execution_prob(s) for s in env.spreads]\n",
    "    optimal_arm = np.argmax(expected_rewards)\n",
    "    optimal_reward = expected_rewards[optimal_arm]\n",
    "\n",
    "    rewards = [[] for _ in range(num_agents)]\n",
    "    regrets = [[] for _ in range(num_agents)]\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        arm_choices = [agent.select_arm() for agent in agents]\n",
    "        reward_list = env.pull_arms_multiagent(arm_choices)\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            agents[i].update(arm_choices[i], reward_list[i])\n",
    "            current_expected = env.spreads[arm_choices[i]] * env.get_execution_prob(env.spreads[arm_choices[i]])\n",
    "            regret = optimal_reward - current_expected\n",
    "            regrets[i].append(regret)\n",
    "            rewards[i].append(reward_list[i])\n",
    "\n",
    "    cum_rewards_list = [np.cumsum(r) for r in rewards]\n",
    "    cum_regrets_list = [np.cumsum(r) for r in regrets]\n",
    "    return cum_rewards_list, cum_regrets_list\n",
    "\n",
    "# ==========================\n",
    "# Experiment Configuration\n",
    "# ==========================\n",
    "\n",
    "spreads = [0.1, 0.3, 0.5, 0.7, 0.9] # Available spreads\n",
    "num_arms = len(spreads) # Number of arms\n",
    "competitor_spread = 0.7 # Competitor's fixed spread\n",
    "env_multi = MultiAgentOTCEnvironment(spreads, competitor_spread, alpha, penalty_mode='linear', adversarial_penalty=0.2)\n",
    "alpha = 0.4 # High alpha means high competitiveness (steep execution prob curve)\n",
    "num_steps = 10000  # Simulation length \n",
    "num_agents = 4\n",
    "\n",
    "# ==========================\n",
    "# Algorithm Parameters\n",
    "# ==========================\n",
    "\n",
    "k = 0.05  # Adaptive ε-Greedy exploration scaling\n",
    "exploration_steps = 100  # Fixed exploration phase length\n",
    "epsilon = 0.1  # Standard ε-Greedy exploration rate\n",
    "c = 2  # UCB exploration weight\n",
    "gamma = 0.1  # EXP3 exploration parameter\n",
    "def epsilon_schedule(t): # ε decay schedule for Decaying ε-Greedy\n",
    "    #return 1 / (t + 1)\n",
    "    #return 1 / np.sqrt(t + 1)\n",
    "    return np.log(t+1)/(t+1)\n",
    "\n",
    "# Run each algorithm\n",
    "# Adaptive ε-Greedy\n",
    "ma_adaptive_epsilon_greedy_reward, ma_adaptive_epsilon_greedy_regret = run_multiagent_experiment(AdaptiveEpsilonGreedy, {'num_arms': num_arms, 'k': k}, env_multi, num_agents, num_steps)\n",
    "# Fixed exploration Greedy\n",
    "ma_fixed_exploration_greedy_reward, ma_fixed_exploration_greedy_regret = run_multiagent_experiment(FixedExplorationThenGreedy, {'num_arms': num_arms, 'exploration_steps': exploration_steps}, env_multi, num_agents, num_steps)\n",
    "# ε-Greedy\n",
    "ma_eps_reward, ma_eps_regret = run_multiagent_experiment(EpsilonGreedy, {'num_arms': num_arms, 'epsilon': epsilon}, env_multi, num_agents, num_steps)\n",
    "#Decaying ε-Greedy\n",
    "ma_decaying_reward, ma_decaying_regret = run_multiagent_experiment(EpsilonGreedyDecaying, {'num_arms': num_arms, 'epsilon_schedule': epsilon_schedule}, env_multi, num_agents, num_steps)\n",
    "# UCB\n",
    "ma_ucb_reward, ma_ucb_regret = run_multiagent_experiment(UCB, {'num_arms': num_arms, 'c': c}, env_multi, num_agents, num_steps)\n",
    "# EXP3\n",
    "ma_exp3_reward, ma_exp3_regret = run_multiagent_experiment(EXP3, {'num_arms': num_arms, 'gamma': gamma}, env_multi, num_agents, num_steps)\n",
    "\n",
    "# ==========================\n",
    "# Visualization\n",
    "# ==========================\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Reward Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.mean(ma_adaptive_epsilon_greedy_reward, axis=0), label=f'Adaptive ε-Greedy (k={k})', linestyle='--')\n",
    "plt.plot(np.mean(ma_fixed_exploration_greedy_reward, axis=0), label=f'Fixed Exploration Greedy (exploration_steps={exploration_steps})')\n",
    "plt.plot(np.mean(ma_decaying_reward, axis=0), label='Decaying Epsilon-Greedy')\n",
    "plt.plot(np.mean(ma_eps_reward, axis=0), label=f'Epsilon-Greedy (ε={epsilon})')\n",
    "plt.plot(np.mean(ma_ucb_reward, axis=0), label=f'UCB (c={c})')\n",
    "plt.plot(np.mean(ma_exp3_reward, axis=0), label=f'EXP3 (γ={gamma})')\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Average Cumulative Reward\")\n",
    "plt.legend()\n",
    "\n",
    "# Regret Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.mean(ma_adaptive_epsilon_greedy_regret, axis=0), label=f'Adaptive ε-Greedy (k={k})', linestyle='--')\n",
    "plt.plot(np.mean(ma_fixed_exploration_greedy_regret, axis=0), label=f'Fixed Exploration Greedy (exploration_steps={exploration_steps})')\n",
    "plt.plot(np.mean(ma_decaying_regret, axis=0), label='Decaying Epsilon-Greedy')\n",
    "plt.plot(np.mean(ma_eps_regret, axis=0), label=f'Epsilon-Greedy (ε={epsilon})')\n",
    "plt.plot(np.mean(ma_ucb_regret, axis=0), label=f'UCB (c={c})')\n",
    "plt.plot(np.mean(ma_exp3_regret, axis=0), label=f'EXP3 (γ={gamma})')\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Average Cumulative Regret\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(f\"Multi-agent Stationary OTC Bandit Performance Comparison ({num_agents} Agents, Competitor Spread = {competitor_spread})\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-agent Non-stationary environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi-Armed Bandit Algorithms in OTC (Order-to-Cover) Market Environment\n",
    "Comparison in a Non-stationary Environment and Multi-agent Learning Scenario\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume that OTCEnvironment and other bandit algorithm classes (AdaptiveEpsilonGreedy, FixedExplorationThenGreedy,\n",
    "# EpsilonGreedy, EpsilonGreedyDecaying, UCB) are already defined and available in the current scope.\n",
    "\n",
    "# ==========================\n",
    "# Non-stationary Environment for Multi-agent Learning\n",
    "# ==========================\n",
    "class NonStationaryMultiAgentOTCEnvironment(OTCEnvironment):\n",
    "    \"\"\"\n",
    "    Non-stationary OTC environment with two modifications:\n",
    "    \n",
    "    1. Dynamic update: Every 'dynamic_interval' steps, the competitor's spread is randomly re-assigned,\n",
    "       simulating changes in market conditions.\n",
    "    2. Adversarial penalty: When an arm is selected frequently (as measured by selection_count),\n",
    "       its reward is penalized to simulate multi-agent interference.\n",
    "    \n",
    "    Parameters:\n",
    "    - dynamic_interval: The fixed interval at which competitor_spread is updated.\n",
    "    - adversarial_penalty: The penalty factor applied to frequently selected arms.\n",
    "    - penalty_mode: The mode of penalty ('linear', 'log', or 'none').\n",
    "    \"\"\"\n",
    "    def __init__(self, spreads, competitor_spread, alpha=0.3, dynamic_interval=1000, adversarial_penalty=0.1, penalty_mode='linear'):\n",
    "        super().__init__(spreads, competitor_spread, alpha)\n",
    "        self.dynamic_interval = dynamic_interval\n",
    "        self.adversarial_penalty = adversarial_penalty\n",
    "        self.penalty_mode = penalty_mode\n",
    "        self.step_counter = 0\n",
    "\n",
    "    def update_competitor(self):\n",
    "        # Dynamic update: every 'dynamic_interval' steps, randomly change competitor_spread.\n",
    "        if self.step_counter % self.dynamic_interval == 0:\n",
    "            old = self.competitor_spread\n",
    "            self.competitor_spread = np.random.choice(self.spreads)\n",
    "            print(f\"[Step {self.step_counter}] Competitor spread changed: {old} ➜ {self.competitor_spread}\")\n",
    "    \n",
    "    def pull_arm(self, arm, selection_count=0):\n",
    "        \"\"\"\n",
    "        Simulate pulling an arm with additional dynamic updates and adversarial penalty.\n",
    "        \n",
    "        Parameters:\n",
    "        - arm: The index of the arm to pull.\n",
    "        - selection_count: The cumulative number of times this arm has been selected (used for computing the penalty).\n",
    "        \"\"\"\n",
    "        self.step_counter += 1\n",
    "        self.update_competitor()\n",
    "        s = self.spreads[arm]\n",
    "        prob = self.get_execution_prob(s)\n",
    "        # Apply adversarial penalty: the reward of a frequently chosen arm is reduced proportionally.\n",
    "        if self.penalty_mode == 'linear':\n",
    "            multiplier = 1.0 / (1.0 + self.adversarial_penalty * selection_count)\n",
    "        elif self.penalty_mode == 'log':\n",
    "            multiplier = 1.0 / (1.0 + self.adversarial_penalty * np.log(1 + selection_count))\n",
    "        elif self.penalty_mode == 'none':\n",
    "            multiplier = 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported penalty mode: {self.penalty_mode}\")\n",
    "        \n",
    "        win = np.random.binomial(1, prob)\n",
    "        return win * s * multiplier\n",
    "\n",
    "# ==========================\n",
    "# Multi-agent Experiment Runner for Non-stationary Environment\n",
    "# ==========================\n",
    "def run_experiment_ns_multi(bandit_class, bandit_params, env_ns, num_steps, num_agents=4):\n",
    "    \"\"\"\n",
    "    Run a multi-agent bandit algorithm experiment in a non-stationary, multi-agent learning environment.\n",
    "    \n",
    "    Parameters:\n",
    "    - bandit_class: The bandit algorithm class to test.\n",
    "    - bandit_params: Dictionary of algorithm parameters.\n",
    "    - env_ns: Instance of the NonStationaryOTCEnvironment.\n",
    "    - num_steps: Number of iterations (each iteration, all agents act once).\n",
    "    - num_agents: Number of agents.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_cum_rewards: Cumulative reward averaged over agents.\n",
    "    - avg_cum_regret: Cumulative regret averaged over agents.\n",
    "    \"\"\"\n",
    "    # Initialize agents\n",
    "    agents = [bandit_class(**bandit_params) for _ in range(num_agents)]\n",
    "    # Global counts for each arm: counts the total selections across all agents (for adversarial penalty)\n",
    "    global_counts = np.zeros(len(env_ns.spreads))\n",
    "    \n",
    "    # Arrays to record each agent's reward and regret per time step\n",
    "    rewards_agents = np.zeros((num_agents, num_steps))\n",
    "    regrets_agents = np.zeros((num_agents, num_steps))\n",
    "    \n",
    "    # Precompute the initial optimal arm based on expected rewards \n",
    "    # (Note: in a non-stationary environment, the optimal arm may change over time; here we use the initial optimal as the benchmark)\n",
    "    expected_rewards = [s * env_ns.get_execution_prob(s) for s in env_ns.spreads]\n",
    "    optimal_arm = np.argmax(expected_rewards)\n",
    "    optimal_reward = expected_rewards[optimal_arm]\n",
    "    \n",
    "    for t in range(num_steps):\n",
    "        for i, agent in enumerate(agents):\n",
    "            arm = agent.select_arm()\n",
    "            # Use global selection count as input for adversarial penalty calculation\n",
    "            reward = env_ns.pull_arm(arm, selection_count=global_counts[arm])\n",
    "            # Update global count: this simulates the cumulative effect of multi-agent competition\n",
    "            global_counts[arm] += 1\n",
    "            agent.update(arm, reward)\n",
    "            \n",
    "            current_expected = env_ns.spreads[arm] * env_ns.get_execution_prob(env_ns.spreads[arm])\n",
    "            regret = optimal_reward - current_expected\n",
    "            regrets_agents[i, t] = regret\n",
    "            # For reward, we use 1 if the optimal arm was selected (for comparison), not the actual reward value\n",
    "            rewards_agents[i, t] = 1 if arm == optimal_arm else 0\n",
    "    \n",
    "    # Compute the average cumulative reward and regret across all agents at each time step\n",
    "    avg_rewards = np.mean(rewards_agents, axis=0)\n",
    "    avg_regrets = np.mean(regrets_agents, axis=0)\n",
    "    avg_cum_rewards = np.cumsum(avg_rewards)\n",
    "    avg_cum_regret = np.cumsum(avg_regrets)\n",
    "    \n",
    "    return avg_cum_rewards, avg_cum_regret\n",
    "\n",
    "# ==========================\n",
    "# Experiment Configuration\n",
    "# ==========================\n",
    "\n",
    "spreads = [0.1, 0.3, 0.5, 0.7, 0.9]  # Available spreads\n",
    "num_arms = len(spreads)              # Number of arms\n",
    "competitor_spread = 0.7              # Initial competitor spread\n",
    "alpha = 0.4                        # High alpha indicates high competitiveness\n",
    "env_ns = NonStationaryMultiAgentOTCEnvironment(   # Create the non-stationary environment\n",
    "    spreads, \n",
    "    competitor_spread=competitor_spread,\n",
    "    alpha=alpha,\n",
    "    dynamic_interval=1000,           # Update competitor spread every 1000 steps\n",
    "    adversarial_penalty=0.1          # Penalty factor for frequently selected arms\n",
    ")\n",
    "num_steps = 10000  # Number of simulation steps\n",
    "num_agents = 4     # Number of agents in the multi-agent scenario\n",
    "\n",
    "# ==========================\n",
    "# Algorithm Parameters\n",
    "# ==========================\n",
    "k = 0.05           # Exploration scaling parameter for Adaptive ε-Greedy\n",
    "exploration_steps = 100  # Number of steps for fixed exploration phase in Fixed Exploration Then Greedy\n",
    "epsilon = 0.1      # Standard ε-Greedy exploration rate\n",
    "c = 2              # UCB exploration weight\n",
    "gamma = 0.1        # EXP3 exploration parameter\n",
    "\n",
    "def epsilon_schedule(t):\n",
    "    # Example decay schedule for decaying ε-Greedy: ε decays with time.\n",
    "    return np.log(t+1) / (t+1)\n",
    "\n",
    "# ==========================\n",
    "# Running Multi-agent Experiments in Non-stationary Environment (Section 3.2)\n",
    "# ==========================\n",
    "\n",
    "# Multi-agent Non-stationary Adaptive ε-Greedy\n",
    "NS_adaptive_epsilon_greedy_reward, NS_adaptive_epsilon_greedy_regret = run_experiment_ns_multi(AdaptiveEpsilonGreedy, {'num_arms': num_arms, 'k': k}, env_ns, num_steps, num_agents)\n",
    "# Multi-agent Non-stationary Fixed Exploration Then Greedy\n",
    "NS_fixed_exploration_greedy_reward, NS_fixed_exploration_greedy_regret = run_experiment_ns_multi(FixedExplorationThenGreedy, {'num_arms': num_arms, 'exploration_steps': exploration_steps}, env_ns, num_steps, num_agents)\n",
    "# Multi-agent Non-stationary Epsilon-Greedy\n",
    "NS_epsilon_greedy_reward, NS_epsilon_greedy_regret = run_experiment_ns_multi(EpsilonGreedy, {'num_arms': num_arms, 'epsilon': epsilon}, env_ns, num_steps, num_agents)\n",
    "# Multi-agent Non-stationary Decaying Epsilon-Greedy\n",
    "NS_decaying_epsilon_greedy_reward, NS_decaying_epsilon_greedy_regret = run_experiment_ns_multi(EpsilonGreedyDecaying, {'num_arms': num_arms, 'epsilon_schedule': epsilon_schedule}, env_ns, num_steps, num_agents)\n",
    "# Multi-agent Non-stationary UCB\n",
    "NS_ucb_reward, NS_ucb_regret = run_experiment_ns_multi(UCB, {'num_arms': num_arms, 'c': c}, env_ns, num_steps, num_agents)\n",
    "# Multi-agent Non-stationary EXP3\n",
    "NS_exp3_reward, NS_exp3_regret = run_experiment_ns_multi(EXP3, {'num_arms': num_arms, 'gamma': gamma}, env_ns, num_steps, num_agents)\n",
    "\n",
    "# ==========================\n",
    "# Visualization\n",
    "# ==========================\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Cumulative Reward Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(NS_adaptive_epsilon_greedy_reward, label=f'Adaptive ε-Greedy NS (k={k})', linestyle='--')\n",
    "plt.plot(NS_fixed_exploration_greedy_reward, label=f'Fixed Exploration NS (exploration_steps={exploration_steps})')\n",
    "plt.plot(NS_epsilon_greedy_reward, label=f'Epsilon-Greedy NS (ε={epsilon})')\n",
    "plt.plot(NS_decaying_epsilon_greedy_reward, label='Decaying ε-Greedy NS')\n",
    "plt.plot(NS_ucb_reward, label=f'UCB NS (c={c})')\n",
    "plt.plot(NS_exp3_reward, label=f'EXP3 NS (γ={gamma})')\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Reward (Avg over Agents)\")\n",
    "plt.legend()\n",
    "\n",
    "# Cumulative Regret Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(NS_adaptive_epsilon_greedy_regret, label=f'Adaptive ε-Greedy NS (k={k})', linestyle='--')\n",
    "plt.plot(NS_fixed_exploration_greedy_regret, label=f'Fixed Exploration NS (exploration_steps={exploration_steps})')\n",
    "plt.plot(NS_epsilon_greedy_regret, label=f'Epsilon-Greedy NS (ε={epsilon})')\n",
    "plt.plot(NS_decaying_epsilon_greedy_regret, label='Decaying ε-Greedy NS')\n",
    "plt.plot(NS_ucb_regret, label=f'UCB NS (c={c})')\n",
    "plt.plot(NS_exp3_regret, label=f'EXP3 NS (γ={gamma})')\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret (Avg over Agents)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"Non-stationary & Multi-agent OTC Bandit Performance Comparison (Agents = 4)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test only\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================\n",
    "# Experiment Configuration\n",
    "# ==========================\n",
    "num_arms = 10\n",
    "spreads = np.linspace(0.1, 1, num_arms)\n",
    "competitor_spread = 0.7\n",
    "alpha = 0.4\n",
    "num_trials = 100\n",
    "num_steps = 10000\n",
    "k_values = np.linspace(0.01, 0.1, 10)  # Example k values\n",
    "\n",
    "# ==========================\n",
    "# Running Trials\n",
    "# ==========================\n",
    "reward_counts = {k: 0 for k in k_values}\n",
    "regret_counts = {k: 0 for k in k_values}\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    trial_rewards = {}\n",
    "    trial_regrets = {}\n",
    "    for k in k_values:\n",
    "        env = OTCEnvironment(spreads, competitor_spread=competitor_spread, alpha=alpha)\n",
    "        cum_rewards, cum_regrets = run_experiment(\n",
    "            AdaptiveEpsilonGreedy,\n",
    "            {'num_arms': num_arms, 'k': k},\n",
    "            env,\n",
    "            num_steps\n",
    "        )\n",
    "        trial_rewards[k] = cum_rewards[-1]\n",
    "        trial_regrets[k] = cum_regrets[-1]\n",
    "    \n",
    "    # Update counts for highest reward\n",
    "    max_reward = max(trial_rewards.values())\n",
    "    best_reward_ks = [k for k, v in trial_rewards.items() if v == max_reward]\n",
    "    reward_counts[best_reward_ks[0]] += 1\n",
    "    \n",
    "    # Update counts for lowest regret\n",
    "    min_regret = min(trial_regrets.values())\n",
    "    best_regret_ks = [k for k, v in trial_regrets.items() if v == min_regret]\n",
    "    regret_counts[best_regret_ks[0]] += 1\n",
    "\n",
    "    if trial % 10 == 0:\n",
    "        print(trial)\n",
    "\n",
    "# ==========================\n",
    "# Creating Results Table\n",
    "# ==========================\n",
    "results = []\n",
    "for k in k_values:\n",
    "    results.append({\n",
    "        'k': f\"{k:.3f}\",\n",
    "        'Reward Wins': reward_counts[k],\n",
    "        'Regret Wins': regret_counts[k]\n",
    "    })\n",
    "\n",
    "# Sort by Regret Wins ascending (lowest regret first)\n",
    "results_df = pd.DataFrame(results).sort_values(by='Regret Wins', ascending=False)\n",
    "\n",
    "print(\"Adaptive Epsilon-Greedy Performance Over 100 Trials\")\n",
    "print(results_df.to_string(index=False))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
