{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes below are usless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original assignment 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DynamicPricingEnv:\n",
    "    def __init__(self, prices, change_points, reward_probabilities):\n",
    "        self.prices = prices\n",
    "        self.change_points = change_points\n",
    "        self.reward_probabilities = reward_probabilities\n",
    "        self.current_phase = 0\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, price_index):\n",
    "        if self.t in self.change_points:\n",
    "            self.current_phase = min(self.current_phase + 1, len(self.reward_probabilities) - 1)\n",
    "        self.t += 1\n",
    "        return np.random.rand() < self.reward_probabilities[self.current_phase][price_index]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_phase = 0\n",
    "        self.t = 0\n",
    "\n",
    "class DiscountedUCB:\n",
    "    def __init__(self, n_arms, gamma=0.99):\n",
    "        self.n_arms = n_arms\n",
    "        self.gamma = gamma\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        # TODO: Implement the arm selection logic for Discounted UCB\n",
    "        return np.random.randint(self.n_arms)  # Temporary fallback to avoid error\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        # TODO: Implement the update rule for Discounted UCB\n",
    "        pass\n",
    "\n",
    "class SlidingWindowUCB:\n",
    "    def __init__(self, n_arms, window_size=50):   # Default window size set to 50\n",
    "        self.n_arms = n_arms\n",
    "        self.window_size = window_size\n",
    "        self.rewards = [[] for _ in range(n_arms)]\n",
    "\n",
    "    def select_arm(self):\n",
    "        # TODO: Implement the arm selection logic for Sliding Window UCB\n",
    "        return np.random.randint(self.n_arms)  # Temporary fallback to avoid error\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        # TODO: Implement update rule for Sliding Window UCB\n",
    "        pass\n",
    "\n",
    "# Experiment Setup\n",
    "prices = [5, 10, 15, 20]\n",
    "change_points = [2000, 4000, 6000, 8000]\n",
    "reward_probabilities = [\n",
    "    [0.3, 0.5, 0.2, 0.1],  # Phase 1\n",
    "    [0.2, 0.6, 0.3, 0.15], # Phase 2\n",
    "    [0.1, 0.4, 0.5, 0.3],  # Phase 3\n",
    "    [0.25, 0.35, 0.3, 0.2], # Phase 4\n",
    "    [0.15, 0.5, 0.25, 0.4]  # Phase 5\n",
    "]\n",
    "\n",
    "env = DynamicPricingEnv(prices, change_points, reward_probabilities)\n",
    "d_ucb = DiscountedUCB(len(prices))\n",
    "sw_ucb = SlidingWindowUCB(len(prices))\n",
    "\n",
    "T = 10000  # Total time steps\n",
    "regrets_d = []\n",
    "regrets_sw = []\n",
    "\n",
    "for t in range(T):\n",
    "    # Discounted UCB\n",
    "    arm_d = d_ucb.select_arm()\n",
    "    reward_d = env.step(arm_d)\n",
    "    d_ucb.update(arm_d, reward_d)\n",
    "    regrets_d.append(max(reward_probabilities[env.current_phase]) - reward_d)\n",
    "\n",
    "    # Sliding Window UCB\n",
    "    arm_sw = sw_ucb.select_arm()\n",
    "    reward_sw = env.step(arm_sw)\n",
    "    sw_ucb.update(arm_sw, reward_sw)\n",
    "    regrets_sw.append(max(reward_probabilities[env.current_phase]) - reward_sw)\n",
    "\n",
    "# Plot Regrets\n",
    "plt.plot(np.cumsum(regrets_d), label='Discounted UCB')\n",
    "plt.plot(np.cumsum(regrets_sw), label='Sliding Window UCB')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of Discounted UCB and Sliding Window UCB in Dynamic Pricing\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT assignment 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DynamicPricingEnv:\n",
    "    def __init__(self, prices, change_points, reward_probabilities):\n",
    "        self.prices = prices\n",
    "        self.change_points = change_points\n",
    "        self.reward_probabilities = reward_probabilities\n",
    "        self.current_phase = 0\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, price_index):\n",
    "        if self.t in self.change_points:\n",
    "            self.current_phase = min(self.current_phase + 1, len(self.reward_probabilities) - 1)\n",
    "        self.t += 1\n",
    "        return np.random.rand() < self.reward_probabilities[self.current_phase][price_index]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_phase = 0\n",
    "        self.t = 0\n",
    "\n",
    "class DiscountedUCB:\n",
    "    def __init__(self, n_arms, gamma=0.99):\n",
    "        self.n_arms = n_arms\n",
    "        self.gamma = gamma\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        total_counts = np.sum(self.counts)\n",
    "        if total_counts == 0:\n",
    "            return np.random.randint(self.n_arms)\n",
    "\n",
    "        ucb_values = self.values + np.sqrt((2 * np.log(total_counts)) / (self.counts + 1e-6))\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts *= self.gamma\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] = ((1 - self.gamma) * self.values[arm]) + (self.gamma * reward)\n",
    "\n",
    "class SlidingWindowUCB:\n",
    "    def __init__(self, n_arms, window_size=50):\n",
    "        self.n_arms = n_arms\n",
    "        self.window_size = window_size\n",
    "        self.rewards = [[] for _ in range(n_arms)]\n",
    "\n",
    "    def select_arm(self):\n",
    "        ucb_values = []\n",
    "        for i in range(self.n_arms):\n",
    "            if len(self.rewards[i]) == 0:\n",
    "                ucb_values.append(float('inf'))\n",
    "            else:\n",
    "                mean_reward = np.mean(self.rewards[i])\n",
    "                confidence = np.sqrt((2 * np.log(sum(len(r) for r in self.rewards) + 1)) / len(self.rewards[i]))\n",
    "                ucb_values.append(mean_reward + confidence)\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.rewards[arm].append(reward)\n",
    "        if len(self.rewards[arm]) > self.window_size:\n",
    "            self.rewards[arm].pop(0)\n",
    "\n",
    "# Experiment Setup\n",
    "prices = [5, 10, 15, 20]\n",
    "change_points = [2000, 4000, 6000, 8000]\n",
    "reward_probabilities = [\n",
    "    [0.3, 0.5, 0.2, 0.1],  # Phase 1\n",
    "    [0.2, 0.6, 0.3, 0.15], # Phase 2\n",
    "    [0.1, 0.4, 0.5, 0.3],  # Phase 3\n",
    "    [0.25, 0.35, 0.3, 0.2], # Phase 4\n",
    "    [0.15, 0.5, 0.25, 0.4]  # Phase 5\n",
    "]\n",
    "\n",
    "env = DynamicPricingEnv(prices, change_points, reward_probabilities)\n",
    "d_ucb = DiscountedUCB(len(prices))\n",
    "sw_ucb = SlidingWindowUCB(len(prices))\n",
    "\n",
    "T = 10000  # Total time steps\n",
    "regrets_d = []\n",
    "regrets_sw = []\n",
    "\n",
    "for t in range(T):\n",
    "    # Discounted UCB\n",
    "    arm_d = d_ucb.select_arm()\n",
    "    reward_d = env.step(arm_d)\n",
    "    d_ucb.update(arm_d, reward_d)\n",
    "    regrets_d.append(max(reward_probabilities[env.current_phase]) - reward_d)\n",
    "\n",
    "    # Sliding Window UCB\n",
    "    arm_sw = sw_ucb.select_arm()\n",
    "    reward_sw = env.step(arm_sw)\n",
    "    sw_ucb.update(arm_sw, reward_sw)\n",
    "    regrets_sw.append(max(reward_probabilities[env.current_phase]) - reward_sw)\n",
    "\n",
    "# Plot Regrets\n",
    "plt.plot(np.cumsum(regrets_d), label='Discounted UCB')\n",
    "plt.plot(np.cumsum(regrets_sw), label='Sliding Window UCB')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of Discounted UCB and Sliding Window UCB in Dynamic Pricing\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original assignment 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bandit Environment\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = reward_means\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.reward_means[arm])  # Bernoulli rewards\n",
    "\n",
    "# ==========================\n",
    "# Fixed Exploration Then Exploitation (TODO: Implement switching strategy)\n",
    "# ==========================\n",
    "class FixedExplorationThenGreedy:\n",
    "    def __init__(self, num_arms, exploration_steps):\n",
    "        self.num_arms = num_arms\n",
    "        self.exploration_steps = exploration_steps\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement fixed exploration for N steps, then greedy \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "# ==========================\n",
    "# Epsilon-Greedy Algorithm (TODO: Complete update function)\n",
    "# ==========================\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, num_arms, epsilon):\n",
    "        self.num_arms = num_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement selection rule \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement incremental mean update \"\"\"\n",
    "        pass\n",
    "\n",
    "# ==========================\n",
    "# Epsilon-Greedy with Decaying Exploration (TODO: Complete decay schedule and compare schedules)\n",
    "# ==========================\n",
    "class EpsilonGreedyDecaying:\n",
    "    def __init__(self, num_arms, epsilon_schedule):\n",
    "        self.num_arms = num_arms\n",
    "        self.epsilon_schedule = epsilon_schedule  # Function for epsilon_t\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement selection rule \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement update rule including epsilon decay \"\"\"\n",
    "        pass\n",
    "\n",
    "# ==========================\n",
    "# UCB Algorithm (TODO: Complete selection function)\n",
    "# ==========================\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement UCB selection rule \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "# ==========================\n",
    "# Thompson Sampling Algorithm (TODO: Implement Thompson Sampling)\n",
    "# ==========================\n",
    "class ThompsonSampling:\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.successes = np.zeros(num_arms)\n",
    "        self.failures = np.zeros(num_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement Thompson Sampling selection rule \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement update rule for Beta distribution \"\"\"\n",
    "        pass\n",
    "\n",
    "# ==========================\n",
    "# Experiment Runner\n",
    "# ==========================\n",
    "def run_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    regrets = []\n",
    "    optimal_reward = max(env.reward_means)\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        bandit.update(arm, reward)\n",
    "        regret = optimal_reward - reward\n",
    "        regrets.append(regret)\n",
    "\n",
    "    return np.cumsum(regrets)\n",
    "\n",
    "# ==========================\n",
    "# Running Experiments\n",
    "# ==========================\n",
    "num_arms = 10\n",
    "reward_means = np.linspace(0, 1, num_arms)  # Linearly spaced rewards\n",
    "env = BanditEnvironment(num_arms, reward_means)\n",
    "num_steps = 10000\n",
    "\n",
    "# Define epsilon schedule\n",
    "def epsilon_schedule(t):\n",
    "    return 1 / (t + 1)\n",
    "\n",
    "# Plot setup\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Run and plot Fixed Exploration Then Exploitation\n",
    "fixed_exploration_regret = run_experiment(FixedExplorationThenGreedy, {'num_arms': num_arms, 'exploration_steps': 100}, env, num_steps)\n",
    "plt.plot(fixed_exploration_regret, label='Fixed Exploration')\n",
    "\n",
    "# Run and plot ε-Greedy\n",
    "epsilon_greedy_regret = run_experiment(EpsilonGreedy, {'num_arms': num_arms, 'epsilon': 0.1}, env, num_steps)\n",
    "plt.plot(epsilon_greedy_regret, label='Epsilon-Greedy')\n",
    "\n",
    "# Run and plot Decaying ε-Greedy\n",
    "decaying_epsilon_greedy_regret = run_experiment(EpsilonGreedyDecaying, {'num_arms': num_arms, 'epsilon_schedule': epsilon_schedule}, env, num_steps)\n",
    "plt.plot(decaying_epsilon_greedy_regret, label='Decaying Epsilon-Greedy')\n",
    "\n",
    "# Run and plot UCB\n",
    "ucb_regret = run_experiment(UCB, {'num_arms': num_arms, 'c': 4}, env, num_steps)\n",
    "plt.plot(ucb_regret, label='UCB')\n",
    "\n",
    "# Run and plot Thompson Sampling\n",
    "thompson_regret = run_experiment(ThompsonSampling, {'num_arms': num_arms}, env, num_steps)\n",
    "plt.plot(thompson_regret, label='Thompson Sampling')\n",
    "\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "plt.title(\"Bandit Algorithm Performance\")\n",
    "plt.show()\n",
    "\n",
    "# ==========================\n",
    "# Instructions for Students\n",
    "# ==========================\n",
    "print(\"TODO: Complete the missing functions for Fixed-Exploration-Greedy, Epsilon-Greedy, Decaying Epsilon-Greedy, UCB, and Thompson Sampling.\")\n",
    "print(\"TODO: Implement and compare different epsilon schedules (e.g., 1/t, 1/sqrt(t), log(t)/t). Discuss the impact on exploration and cumulative regret in your report.\")\n",
    "print(\"TODO: Answer the questions in the assignment and conduct the necessary experiments to answer them.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT assignment 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bandit Environment\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = reward_means\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.reward_means[arm])  # Bernoulli rewards\n",
    "\n",
    "# ==========================\n",
    "# Fixed Exploration Then Exploitation (TODO: Implement switching strategy)\n",
    "# ==========================\n",
    "class FixedExplorationThenGreedy:\n",
    "    def __init__(self, num_arms, exploration_steps):\n",
    "        self.num_arms = num_arms\n",
    "        self.exploration_steps = exploration_steps\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement fixed exploration for N steps, then greedy \"\"\"\n",
    "        if self.t <= self.exploration_steps:\n",
    "            return np.random.randint(self.num_arms)  # Random exploration\n",
    "        return np.argmax(self.values)  # Greedy exploitation\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "# ==========================\n",
    "# Epsilon-Greedy Algorithm (TODO: Complete update function)\n",
    "# ==========================\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, num_arms, epsilon):\n",
    "        self.num_arms = num_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement selection rule \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.num_arms)  # Explore\n",
    "        return np.argmax(self.values)  # Exploit\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement incremental mean update \"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "\n",
    "# ==========================\n",
    "# Epsilon-Greedy with Decaying Exploration (TODO: Complete decay schedule and compare schedules)\n",
    "# ==========================\n",
    "class EpsilonGreedyDecaying:\n",
    "    def __init__(self, num_arms, epsilon_schedule):\n",
    "        self.num_arms = num_arms\n",
    "        self.epsilon_schedule = epsilon_schedule  # Function for epsilon_t\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement selection rule \"\"\"\n",
    "        epsilon_t = self.epsilon_schedule(self.t)\n",
    "        if np.random.rand() < epsilon_t:\n",
    "            return np.random.randint(self.num_arms)  # Explore\n",
    "        return np.argmax(self.values)  # Exploit\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement update rule including epsilon decay \"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "# ==========================\n",
    "# UCB Algorithm (TODO: Complete selection function)\n",
    "# ==========================\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" Implement UCB selection rule \"\"\"\n",
    "        total_counts = np.sum(self.counts)\n",
    "        if total_counts < self.num_arms:  # Ensure each arm is tried at least once\n",
    "            return int(total_counts)  # Explicitly cast to integer\n",
    "        ucb_values = self.values + self.c * np.sqrt(np.log(total_counts) / (self.counts + 1e-6))\n",
    "        return int(np.argmax(ucb_values))  # Ensure output is an integer\n",
    "\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "# ==========================\n",
    "# Thompson Sampling Algorithm (TODO: Implement Thompson Sampling)\n",
    "# ==========================\n",
    "class ThompsonSampling:\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.successes = np.zeros(num_arms)\n",
    "        self.failures = np.zeros(num_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\" TODO: Implement Thompson Sampling selection rule \"\"\"\n",
    "        samples = [np.random.beta(self.successes[i] + 1, self.failures[i] + 1) for i in range(self.num_arms)]\n",
    "        return np.argmax(samples)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        \"\"\" TODO: Implement update rule for Beta distribution \"\"\"\n",
    "        if reward == 1:\n",
    "            self.successes[arm] += 1\n",
    "        else:\n",
    "            self.failures[arm] += 1\n",
    "\n",
    "# ==========================\n",
    "# Experiment Runner\n",
    "# ==========================\n",
    "def run_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    regrets = []\n",
    "    optimal_reward = max(env.reward_means)\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        bandit.update(arm, reward)\n",
    "        regret = optimal_reward - reward\n",
    "        regrets.append(regret)\n",
    "\n",
    "    return np.cumsum(regrets)\n",
    "\n",
    "# ==========================\n",
    "# Running Experiments\n",
    "# ==========================\n",
    "num_arms = 10\n",
    "reward_means = np.linspace(0, 1, num_arms)  # Linearly spaced rewards\n",
    "env = BanditEnvironment(num_arms, reward_means)\n",
    "num_steps = 10000\n",
    "\n",
    "# Define epsilon schedule\n",
    "def epsilon_schedule(t):\n",
    "    return 1 / (t + 1)\n",
    "\n",
    "# Plot setup\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Run and plot Fixed Exploration Then Exploitation\n",
    "fixed_exploration_regret = run_experiment(FixedExplorationThenGreedy, {'num_arms': num_arms, 'exploration_steps': 100}, env, num_steps)\n",
    "plt.plot(fixed_exploration_regret, label='Fixed Exploration')\n",
    "\n",
    "# Run and plot ε-Greedy\n",
    "epsilon_greedy_regret = run_experiment(EpsilonGreedy, {'num_arms': num_arms, 'epsilon': 0.1}, env, num_steps)\n",
    "plt.plot(epsilon_greedy_regret, label='Epsilon-Greedy')\n",
    "\n",
    "# Run and plot Decaying ε-Greedy\n",
    "decaying_epsilon_greedy_regret = run_experiment(EpsilonGreedyDecaying, {'num_arms': num_arms, 'epsilon_schedule': epsilon_schedule}, env, num_steps)\n",
    "plt.plot(decaying_epsilon_greedy_regret, label='Decaying Epsilon-Greedy')\n",
    "\n",
    "# Run and plot UCB\n",
    "ucb_regret = run_experiment(UCB, {'num_arms': num_arms, 'c': 4}, env, num_steps)\n",
    "plt.plot(ucb_regret, label='UCB')\n",
    "\n",
    "# Run and plot Thompson Sampling\n",
    "thompson_regret = run_experiment(ThompsonSampling, {'num_arms': num_arms}, env, num_steps)\n",
    "plt.plot(thompson_regret, label='Thompson Sampling')\n",
    "\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "plt.title(\"Bandit Algorithm Performance\")\n",
    "plt.show()\n",
    "\n",
    "# ==========================\n",
    "# Instructions for Students\n",
    "# ==========================\n",
    "print(\"TODO: Complete the missing functions for Fixed-Exploration-Greedy, Epsilon-Greedy, Decaying Epsilon-Greedy, UCB, and Thompson Sampling.\")\n",
    "print(\"TODO: Implement and compare different epsilon schedules (e.g., 1/t, 1/sqrt(t), log(t)/t). Discuss the impact on exploration and cumulative regret in your report.\")\n",
    "print(\"TODO: Answer the questions in the assignment and conduct the necessary experiments to answer them.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = reward_means\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.reward_means[arm])\n",
    "\n",
    "class DynamicBanditEnvironment(BanditEnvironment):\n",
    "    def __init__(self, num_arms, reward_means, volatility=0.001):\n",
    "        super().__init__(num_arms, reward_means)\n",
    "        self.volatility = volatility\n",
    "        self.original_means = reward_means.copy()\n",
    "        \n",
    "    def pull_arm(self, arm):\n",
    "        # Simulate dynamic environment with mean reversion\n",
    "        self.reward_means += np.random.normal(0, self.volatility, self.num_arms)\n",
    "        self.reward_means = np.clip(self.reward_means, 0, 1)\n",
    "        return np.random.binomial(1, self.reward_means[arm])\n",
    "\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        #for arm in range(self.num_arms):\n",
    "        #    if self.counts[arm] == 0:\n",
    "        #        return arm\n",
    "\n",
    "        if np.any(self.counts == 0):  # Ensure each arm is picked once\n",
    "            untried_arms = np.where(self.counts == 0)[0]\n",
    "            return np.random.choice(untried_arms)  # Randomize initial selections\n",
    "\n",
    "        ucb_values = self.values + self.c * np.sqrt(np.log(self.t) / self.counts)\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "class EXP3:\n",
    "    def __init__(self, num_arms, gamma):\n",
    "        self.num_arms = num_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + \\\n",
    "                       self.gamma / self.num_arms\n",
    "        return np.random.choice(self.num_arms, p=probabilities)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + \\\n",
    "                       self.gamma / self.num_arms\n",
    "        estimated_reward = reward / probabilities[arm]\n",
    "        self.weights[arm] *= np.exp(self.gamma * estimated_reward / self.num_arms)\n",
    "        self.t += 1\n",
    "\n",
    "def run_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    regrets = []\n",
    "    optimal_reward = max(env.reward_means if isinstance(env, BanditEnvironment) else env.original_means)\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        bandit.update(arm, reward)\n",
    "\n",
    "        regret = optimal_reward - reward\n",
    "        regrets.append(regret)\n",
    "\n",
    "    return np.cumsum(regrets)\n",
    "\n",
    "def run_multi_agent_experiment(bandit_classes, bandit_params_list, env, num_steps):\n",
    "    bandits = [cls(**params) for cls, params in zip(bandit_classes, bandit_params_list)]\n",
    "    regrets = {i: [] for i in range(len(bandits))}\n",
    "    optimal_reward = max(env.original_means)\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        for i, bandit in enumerate(bandits):\n",
    "            arm = bandit.select_arm()\n",
    "            reward = env.pull_arm(arm)\n",
    "            bandit.update(arm, reward)\n",
    "            regret = optimal_reward - reward\n",
    "            regrets[i].append(regret)\n",
    "\n",
    "    return {k: np.cumsum(v) for k, v in regrets.items()}\n",
    "\n",
    "# Experimental setup\n",
    "num_arms = 10\n",
    "base_reward_means = np.linspace(0, 1, num_arms)\n",
    "num_steps = 10000\n",
    "\n",
    "# Single-Agent Static Environment\n",
    "static_env = BanditEnvironment(num_arms, base_reward_means.copy())\n",
    "\n",
    "# Multi-Agent Dynamic Environment\n",
    "dynamic_env = DynamicBanditEnvironment(num_arms, base_reward_means.copy(), volatility=0.002)\n",
    "\n",
    "# Run experiments\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Single-Agent Experiments\n",
    "plt.subplot(1, 2, 1)\n",
    "ucb_static_regret = run_experiment(UCB, {'num_arms': num_arms, 'c': 2}, static_env, num_steps)\n",
    "exp3_static_regret = run_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, static_env, num_steps)\n",
    "\n",
    "plt.plot(ucb_static_regret, label='UCB (Static)')\n",
    "plt.plot(exp3_static_regret, label='EXP3 (Static)')\n",
    "plt.title(\"Single-Agent Static Environment\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Multi-Agent Dynamic Environment\n",
    "plt.subplot(1, 2, 2)\n",
    "dynamic_results = run_multi_agent_experiment(\n",
    "    [UCB, EXP3, UCB, EXP3],\n",
    "    [{'num_arms': num_arms, 'c': 2},\n",
    "     {'num_arms': num_arms, 'gamma': 0.1},\n",
    "     {'num_arms': num_arms, 'c': 2},\n",
    "     {'num_arms': num_arms, 'gamma': 0.1}],\n",
    "    dynamic_env,\n",
    "    num_steps\n",
    ")\n",
    "\n",
    "# Average results for each algorithm type\n",
    "ucb_dynamic_regret = (dynamic_results[0] + dynamic_results[2]) / 2\n",
    "exp3_dynamic_regret = (dynamic_results[1] + dynamic_results[3]) / 2\n",
    "\n",
    "plt.plot(ucb_dynamic_regret, label='UCB (Dynamic)')\n",
    "plt.plot(exp3_dynamic_regret, label='EXP3 (Dynamic)')\n",
    "plt.title(\"Multi-Agent Dynamic Environment\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TrackingBanditEnvironment(BanditEnvironment):\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        super().__init__(num_arms, reward_means)\n",
    "        self.optimal_arm = np.argmax(reward_means)\n",
    "        \n",
    "# Modified experiment function to track arm probabilities\n",
    "def run_tracking_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    arm_selections = np.zeros((num_steps, env.num_arms))\n",
    "    \n",
    "    for t in range(num_steps):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        bandit.update(arm, reward)\n",
    "        arm_selections[t, arm] = 1\n",
    "    \n",
    "    # Calculate cumulative probabilities\n",
    "    cumulative_counts = np.cumsum(arm_selections, axis=0)\n",
    "    total_counts = np.arange(1, num_steps+1)[:, np.newaxis]\n",
    "    return cumulative_counts / total_counts\n",
    "\n",
    "# Set up environment with clear optimal arm (mirroring paper's s values)\n",
    "num_arms = 5\n",
    "reward_means = np.array([0.1, 0.3, 0.5, 0.7, 0.9])  # Explicit s values (spreads)\n",
    "static_env = TrackingBanditEnvironment(num_arms, reward_means)\n",
    "num_steps = 10000\n",
    "\n",
    "# Run tracking experiments\n",
    "ucb_probs = run_tracking_experiment(UCB, {'num_arms': num_arms, 'c': 2}, static_env, num_steps)\n",
    "exp3_probs = run_tracking_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, static_env, num_steps)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# UCB Arm Selection Probabilities\n",
    "plt.subplot(1, 2, 1)\n",
    "for arm in range(num_arms):\n",
    "    plt.plot(ucb_probs[:, arm], label=f'Arm {arm+1} (s={reward_means[arm]:.1f})', alpha=0.8)\n",
    "plt.axhline(y=0.9, color='black', linestyle='--', alpha=0.5)\n",
    "plt.title('UCB: Arm Selection Probability Over Time')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability of Arm Draw')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# EXP3 Arm Selection Probabilities\n",
    "plt.subplot(1, 2, 2)\n",
    "for arm in range(num_arms):\n",
    "    plt.plot(exp3_probs[:, arm], label=f'Arm {arm+1} (s={reward_means[arm]:.1f})', alpha=0.8)\n",
    "plt.axhline(y=0.9, color='black', linestyle='--', alpha=0.5)\n",
    "plt.title('EXP3: Arm Selection Probability Over Time')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability of Arm Draw')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, reward_means):\n",
    "        self.num_arms = num_arms\n",
    "        self.reward_means = reward_means\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.binomial(1, self.reward_means[arm])\n",
    "\n",
    "class FixedCompetitorEnvironment(BanditEnvironment):\n",
    "    def __init__(self, num_arms, reward_means, competitor_arm):\n",
    "        super().__init__(num_arms, reward_means)\n",
    "        self.competitor_arm = competitor_arm\n",
    "        self.competitor_value = reward_means[competitor_arm]\n",
    "        \n",
    "    def pull_arm(self, arm):\n",
    "        # Reward is probability of winning against competitor\n",
    "        return np.random.binomial(1, self.reward_means[arm] > self.competitor_value)\n",
    "\n",
    "class UCB:\n",
    "    def __init__(self, num_arms, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(num_arms)\n",
    "        self.values = np.zeros(num_arms)\n",
    "        self.t = 1\n",
    "\n",
    "    def select_arm(self):\n",
    "        if np.any(self.counts == 0):\n",
    "            return np.random.choice(np.where(self.counts == 0)[0])\n",
    "        ucb_values = self.values + self.c * np.sqrt(np.log(self.t) / (self.counts + 1e-6))\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "        self.t += 1\n",
    "\n",
    "class EXP3:\n",
    "    def __init__(self, num_arms, gamma):\n",
    "        self.num_arms = num_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(num_arms)\n",
    "        \n",
    "    def select_arm(self):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + self.gamma / self.num_arms\n",
    "        return np.random.choice(self.num_arms, p=probabilities)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        probabilities = (1 - self.gamma) * (self.weights / np.sum(self.weights)) + self.gamma / self.num_arms\n",
    "        estimated_reward = reward / probabilities[arm]\n",
    "        self.weights[arm] *= np.exp(self.gamma * estimated_reward / self.num_arms)\n",
    "\n",
    "\n",
    "def run_tracking_experiment(bandit_class, bandit_params, env, num_steps):\n",
    "    bandit = bandit_class(**bandit_params)\n",
    "    arm_selections = np.zeros((num_steps, env.num_arms))\n",
    "    \n",
    "    for t in range(num_steps):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = env.pull_arm(arm)\n",
    "        bandit.update(arm, reward)\n",
    "        arm_selections[t, arm] = 1\n",
    "    \n",
    "    return np.cumsum(arm_selections, axis=0) / np.arange(1, num_steps+1)[:, np.newaxis]\n",
    "\n",
    "# Experimental setup\n",
    "num_arms = 5\n",
    "reward_means = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "competitor_arm = 3  # Competitor selects arm index 3 (s = 0.3)\n",
    "num_steps = 5000\n",
    "\n",
    "# Create environment with fixed competitor\n",
    "env = FixedCompetitorEnvironment(num_arms, reward_means, competitor_arm)\n",
    "\n",
    "# Run experiments\n",
    "ucb_probs = run_tracking_experiment(UCB, {'num_arms': num_arms, 'c': 2}, env, num_steps)\n",
    "exp3_probs = run_tracking_experiment(EXP3, {'num_arms': num_arms, 'gamma': 0.1}, env, num_steps)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# UCB Arm Selection Probabilities\n",
    "plt.subplot(1, 2, 1)\n",
    "for arm in range(num_arms):\n",
    "    plt.plot(ucb_probs[:, arm], label=f'Arm {arm+1} (p={reward_means[arm]:.1f})', alpha=0.8)\n",
    "plt.title('UCB: Probability of Arm Selection Over Time')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# EXP3 Arm Selection Probabilities\n",
    "plt.subplot(1, 2, 2)\n",
    "for arm in range(num_arms):\n",
    "    plt.plot(exp3_probs[:, arm], label=f'Arm {arm+1} (p={reward_means[arm]:.1f})', alpha=0.8)\n",
    "plt.title('EXP3: Probability of Arm Selection Over Time')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "class CompetitiveBanditEnvironment:\n",
    "    def __init__(self, spreads, competitor_spread, s_T=1.0, ω=0.15, ρ=0.5):\n",
    "        \"\"\"\n",
    "        Parameters from paper:\n",
    "        - s_T: Trader's reservation spread\n",
    "        - ω: Measurement error standard deviation\n",
    "        - ρ: Correlation between LPs' measurement errors\n",
    "        \"\"\"\n",
    "        self.spreads = spreads\n",
    "        self.competitor_spread = competitor_spread\n",
    "        self.s_T = s_T\n",
    "        self.ω = ω\n",
    "        self.ρ = ρ\n",
    "        \n",
    "        # Precompute terms from paper's equation (11)\n",
    "        self.ω_T = ω * np.sqrt(ρ + (1-ρ)/2)  # For N=2 competitors\n",
    "        \n",
    "    def get_win_probability(self, s_i):\n",
    "        \"\"\"Calculate probability of winning against competitor (equation 11)\"\"\"\n",
    "        # Simplified version of the paper's integral formulation\n",
    "        # Using probit approximation for the cumulative normal\n",
    "        z = (self.s_T - s_i) / (2 * self.ω_T)\n",
    "        return norm.cdf(z) * norm.cdf((self.competitor_spread - s_i)/(2*self.ω*np.sqrt(1-self.ρ)))\n",
    "    \n",
    "    def get_reward(self, arm):\n",
    "        \"\"\"Full reward calculation from paper's equation (7)\"\"\"\n",
    "        s_i = self.spreads[arm]\n",
    "        win_prob = self.get_win_probability(s_i)\n",
    "        \n",
    "        # Expected reward components\n",
    "        spread_reward = s_i/2 * win_prob\n",
    "        adverse_selection = -self.ω * np.sqrt(1-self.ρ) * (1/np.sqrt(2*np.pi))  # Simplified μ_i^(1)\n",
    "        \n",
    "        # Add noise for exploration\n",
    "        noise = np.random.normal(0, 0.05)\n",
    "        return spread_reward + adverse_selection + noise\n",
    "\n",
    "class UCB:\n",
    "    def __init__(self, n_arms, c=1.5):\n",
    "        self.n_arms = n_arms\n",
    "        self.c = c\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "        self.total_counts = 0\n",
    "        \n",
    "    def select_arm(self):\n",
    "        if self.total_counts < self.n_arms:\n",
    "            return self.total_counts  # Initial exploration\n",
    "        \n",
    "        ucb = self.values + self.c * np.sqrt(np.log(self.total_counts)/(self.counts + 1e-6))\n",
    "        return np.argmax(ucb)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.total_counts += 1\n",
    "        self.values[arm] += (reward - self.values[arm])/self.counts[arm]\n",
    "\n",
    "class EXP3:\n",
    "    def __init__(self, n_arms, gamma=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(n_arms)\n",
    "        self.probabilities = np.ones(n_arms)/n_arms\n",
    "        \n",
    "    def select_arm(self):\n",
    "        self.probabilities = (1-self.gamma)*self.weights/self.weights.sum() + self.gamma/self.n_arms\n",
    "        return np.random.choice(self.n_arms, p=self.probabilities)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        # Normalize reward to [-1,1] for stability\n",
    "        normalized_reward = (reward - 0.2) / 0.5  # Based on observed reward range\n",
    "        estimated_reward = normalized_reward / self.probabilities[arm]\n",
    "        self.weights[arm] *= np.exp(self.gamma * estimated_reward/self.n_arms)\n",
    "        self.weights = np.clip(self.weights, 1e-6, 1e6)\n",
    "\n",
    "# Experimental setup\n",
    "spreads = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "competitor_spread = 0.7\n",
    "env = CompetitiveBanditEnvironment(spreads=spreads,\n",
    "                                  competitor_spread=competitor_spread,\n",
    "                                  s_T=1.0,\n",
    "                                  ω=0.15,\n",
    "                                  ρ=0.5)\n",
    "\n",
    "# Run experiments\n",
    "n_steps = 10000\n",
    "n_runs = 20\n",
    "\n",
    "def run_experiment(algorithm, env):\n",
    "    history = np.zeros((n_steps, len(spreads)))\n",
    "    for t in range(n_steps):\n",
    "        arm = algorithm.select_arm()\n",
    "        reward = env.get_reward(arm)\n",
    "        algorithm.update(arm, reward)\n",
    "        \n",
    "        # Track empirical probabilities\n",
    "        if isinstance(algorithm, UCB):\n",
    "            probs = algorithm.counts / (algorithm.total_counts + 1e-6)\n",
    "        else:\n",
    "            probs = algorithm.probabilities\n",
    "        history[t] = probs\n",
    "    return history\n",
    "\n",
    "# Aggregate results\n",
    "ucb_probs = np.zeros((n_steps, len(spreads)))\n",
    "exp3_probs = np.zeros((n_steps, len(spreads)))\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    ucb = UCB(n_arms=len(spreads))\n",
    "    exp3 = EXP3(n_arms=len(spreads))\n",
    "    \n",
    "    ucb_probs += run_experiment(ucb, env)\n",
    "    exp3_probs += run_experiment(exp3, env)\n",
    "\n",
    "ucb_probs /= n_runs\n",
    "exp3_probs /= n_runs\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, s in enumerate(spreads):\n",
    "    plt.plot(ucb_probs[:, i], label=f's={s:.1f}')\n",
    "plt.title('UCB: Arm Selection Probabilities')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, s in enumerate(spreads):\n",
    "    plt.plot(exp3_probs[:, i], label=f's={s:.1f}')\n",
    "plt.title('EXP3: Arm Selection Probabilities')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import dblquad\n",
    "from functools import lru_cache\n",
    "\n",
    "class OTCMarketBandit:\n",
    "    \"\"\"\n",
    "    Bandit environment implementing the OTC market dynamics from the paper\n",
    "    Key differences from standard bandits:\n",
    "    1. Rewards depend on both spread selection and hidden competitor spreads\n",
    "    2. Non-stationary due to competitor reactions (simulated via fixed spread)\n",
    "    3. Reward includes adverse selection costs (Winner's Curse)\n",
    "    \"\"\"\n",
    "    def __init__(self, arms, competitor_spread=0.7, s_T=1.0, ω=0.15, ρ=0.5):\n",
    "        \"\"\"\n",
    "        :param arms: array of candidate spreads (arms)\n",
    "        :param competitor_spread: fixed spread of competitor (s_2 in paper)\n",
    "        :param s_T: trader's reservation spread\n",
    "        :param ω: volatility of price estimates\n",
    "        :param ρ: correlation between LPs' price estimates\n",
    "        \"\"\"\n",
    "        self.arms = arms\n",
    "        self.s_2 = competitor_spread\n",
    "        self.s_T = s_T\n",
    "        self.ω = ω\n",
    "        self.ρ = ρ\n",
    "        self.ω_T = ω * np.sqrt(ρ + (1-ρ)/2)  # Trader's estimation error\n",
    "        \n",
    "        # Precompute theoretical values for faster access\n",
    "        self.θ_values = np.array([self._compute_θ(s_i) for s_i in arms])\n",
    "        self.μ_values = np.array([self._compute_μ(s_i) for s_i in arms])\n",
    "        \n",
    "    @lru_cache(maxsize=None)\n",
    "    def _compute_θ(self, s_i):\n",
    "        \"\"\"Compute θ_i^(1) from equation (9)\"\"\"\n",
    "        def integrand(x, y):\n",
    "            z1 = (self.ω*np.sqrt(self.ρ)*x + self.ω*np.sqrt(1-self.ρ)*y)/self.ω_T + (self.s_T - s_i)/(2*self.ω_T)\n",
    "            z2 = y + (self.s_2 - s_i)/(2*self.ω*np.sqrt(1-self.ρ))\n",
    "            return norm.cdf(z1) * norm.cdf(z2) * norm.pdf(x) * norm.pdf(y)\n",
    "        return dblquad(integrand, -np.inf, np.inf, -np.inf, np.inf)[0]\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def _compute_μ(self, s_i):\n",
    "        \"\"\"Compute μ_i^(1) from equation (10)\"\"\"\n",
    "        def integrand(x, y):\n",
    "            z = y + np.sqrt(self.ρ/(1-self.ρ))*x\n",
    "            return z * self._compute_θ(s_i) * norm.pdf(x) * norm.pdf(y)\n",
    "        return dblquad(integrand, -np.inf, np.inf, -np.inf, np.inf)[0]\n",
    "\n",
    "    def get_reward(self, arm):\n",
    "        \"\"\"\n",
    "        Returns stochastic reward for selected arm (spread)\n",
    "        Implements equation (7) from the paper:\n",
    "        π_i = s_i/2 * θ_i^(1) - ω√(1-ρ) * μ_i^(1) + noise\n",
    "        \"\"\"\n",
    "        s_i = self.arms[arm]\n",
    "        θ = self.θ_values[arm]\n",
    "        μ = self.μ_values[arm]\n",
    "        deterministic = 0.5*s_i*θ - self.ω*np.sqrt(1-self.ρ)*μ\n",
    "        return deterministic + np.random.normal(0, 0.01)  # Add small noise\n",
    "\n",
    "class UCB1:\n",
    "    \"\"\"\n",
    "    Modified UCB1 algorithm adapted for OTC market dynamics\n",
    "    Key adjustments:\n",
    "    - Tuned exploration parameter for financial spreads\n",
    "    - Optimized for delayed feedback characteristics\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, c=2):\n",
    "        self.n_arms = n_arms\n",
    "        self.c = c  # Exploration parameter\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "        \n",
    "    def select_arm(self):\n",
    "        if np.any(self.counts == 0):\n",
    "            return np.argmax(self.counts == 0)  # Force initial exploration\n",
    "        ucb = self.values + self.c * np.sqrt(np.log(np.sum(self.counts)) / (self.counts + 1e-6))\n",
    "        return np.argmax(ucb)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n",
    "\n",
    "class EXP3:\n",
    "    \"\"\"\n",
    "    EXP3 implementation with financial market adaptations:\n",
    "    - Reward normalization for stability\n",
    "    - Optimized exploration rate for spread environments\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms, gamma=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.gamma = gamma\n",
    "        self.weights = np.ones(n_arms)\n",
    "        self.probs = np.ones(n_arms)/n_arms\n",
    "        \n",
    "    def select_arm(self):\n",
    "        self.probs = (1-self.gamma)*self.weights/self.weights.sum() + self.gamma/self.n_arms\n",
    "        return np.random.choice(self.n_arms, p=self.probs)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        normalized_reward = (reward - 0.2) / 0.5  # Based on observed reward range\n",
    "        estimated = normalized_reward / self.probs[arm]\n",
    "        self.weights[arm] *= np.exp(self.gamma * estimated / self.n_arms)\n",
    "        self.weights = np.clip(self.weights, 1e-6, 1e6)  # Prevent overflow\n",
    "\n",
    "def run_experiment(bandit, algorithm, steps=5000):\n",
    "    \"\"\"Run bandit algorithm in environment\"\"\"\n",
    "    history = np.zeros((steps, len(bandit.arms)))\n",
    "    for t in range(steps):\n",
    "        arm = algorithm.select_arm()\n",
    "        reward = bandit.get_reward(arm)\n",
    "        algorithm.update(arm, reward)\n",
    "        \n",
    "        # Track empirical probabilities\n",
    "        if isinstance(algorithm, UCB1):\n",
    "            probs = algorithm.counts / (np.sum(algorithm.counts) + 1e-6)\n",
    "        else:\n",
    "            probs = algorithm.probs\n",
    "        history[t] = probs\n",
    "    return history\n",
    "\n",
    "# Parameters matching paper's baseline\n",
    "arms = np.array([0.1, 0.3, 0.5, 0.7, 0.9])  # Candidate spreads\n",
    "competitor_spread = 0.7  # Fixed competitor spread (s_2)\n",
    "steps = 5000\n",
    "trials = 20\n",
    "\n",
    "# Initialize environment and algorithms\n",
    "bandit = OTCMarketBandit(arms, competitor_spread)\n",
    "ucb = UCB1(len(arms))\n",
    "exp3 = EXP3(len(arms))\n",
    "\n",
    "# Run experiments\n",
    "ucb_results = np.zeros((steps, len(arms)))\n",
    "exp3_results = np.zeros((steps, len(arms)))\n",
    "\n",
    "for _ in range(trials):\n",
    "    ucb_results += run_experiment(bandit, UCB1(len(arms)), steps)\n",
    "    exp3_results += run_experiment(bandit, EXP3(len(arms)), steps)\n",
    "\n",
    "ucb_results /= trials\n",
    "exp3_results /= trials\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(len(arms)):\n",
    "    plt.plot(ucb_results[:, i], label=f's={arms[i]:.1f}')\n",
    "plt.title('UCB: Arm Selection Probability')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(len(arms)):\n",
    "    plt.plot(exp3_results[:, i], label=f's={arms[i]:.1f}')\n",
    "plt.title('EXP3: Arm Selection Probability')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
