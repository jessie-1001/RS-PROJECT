{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题设定：\n",
    "\n",
    "4个定价选项（0.1, 0.2, 0.3, 0.4），对应转化率（0.9, 0.6, 0.4, 0.2）。\n",
    "\n",
    "每次选择价格后，根据转化概率获得收益（成功返回价格，否则0）。\n",
    "\n",
    "目标是最大化累计收益，最优选项是期望收益（价格×转化率）最高的0.2和0.3（均为0.12）。\n",
    "\n",
    "算法实现：\n",
    "\n",
    "UCB算法：\n",
    "\n",
    "维护每个动作的选择次数n_k和累计奖励r_k。\n",
    "\n",
    "选择UCB值（平均奖励 + 探索项）最大的动作。\n",
    "\n",
    "探索项随着时间增加而减小，平衡探索与利用。\n",
    "\n",
    "EXP3算法：\n",
    "\n",
    "维护权重向量，根据权重计算选择概率（带探索因子γ）。\n",
    "\n",
    "使用重要性采样估计奖励，更新选中动作的权重。\n",
    "\n",
    "γ控制探索程度，γ越大探索越强。\n",
    "\n",
    "模拟流程：\n",
    "\n",
    "每个算法独立运行10次（n_agents=10），每次1000步。\n",
    "\n",
    "记录每个时间步各动作的选择次数，计算平均概率。\n",
    "\n",
    "累积平均概率反映算法的收敛过程。\n",
    "\n",
    "结果可视化：\n",
    "\n",
    "绘制两种算法各动作的选择概率随时间变化曲线（对数时间轴）。\n",
    "\n",
    "理想情况下，算法应快速收敛到最优动作（0.2和0.3），但由于两者期望相同，可能呈现相近概率。\n",
    "\n",
    "关键观察：\n",
    "\n",
    "UCB：初期快速收敛，但因需持续探索，可能保持对两个最优动作的选择。\n",
    "\n",
    "EXP3：探索因子γ=0.3导致更多探索，收敛较慢，但适应非平稳环境更强。\n",
    "\n",
    "改进建议：\n",
    "\n",
    "调整EXP3的γ参数（如随时间衰减）以平衡探索与利用。\n",
    "\n",
    "增加运行次数（n_agents）或时间步（T）提高稳定性。\n",
    "\n",
    "检查多最优情况下的算法行为，验证是否合理分配选择概率。\n",
    "\n",
    "结论：该代码有效对比了UCB和EXP3在动态定价中的表现，UCB可能更快锁定最优选项，而EXP3更适合对抗性环境。实际应用需根据问题特性选择算法"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
